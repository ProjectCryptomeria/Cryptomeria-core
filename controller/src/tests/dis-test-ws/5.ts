import { stringToPath } from '@cosmjs/crypto';
import { AccountData, DirectSecp256k1HdWallet, EncodeObject, GeneratedType, Registry, } from '@cosmjs/proto-signing';
import { calculateFee, GasPrice, SigningStargateClient } from '@cosmjs/stargate';
import { Comet38Client, WebsocketClient } from '@cosmjs/tendermint-rpc';
import { TxEvent } from "@cosmjs/tendermint-rpc/build/comet38/responses";
import { sleep } from "@cosmjs/utils";
import * as k8s from '@kubernetes/client-node';
import cliProgress from 'cli-progress';
import { TxRaw } from 'cosmjs-types/cosmos/tx/v1beta1/tx';
import * as fs from 'fs'; // fs.stat ã®ãŸã‚ã«ä½¿ç”¨
import * as path from 'path';
import { Reader, Writer } from 'protobufjs/minimal';
import winston from 'winston';
import Transport from 'winston-transport';
import { Listener, Stream } from "xstream";

// =================================================================================================
// ğŸ“š I. CONFIG & TYPE DEFINITIONS
// =================================================================================================

/**
 * ã™ã¹ã¦ã®è¨­å®šå€¤ã‚’ã“ã“ã«é›†ç´„
 */
const CONFIG = {
	K8S_NAMESPACE: 'raidchain',
	SECRET_NAME: 'raidchain-mnemonics',
	BLOCK_SIZE_LIMIT_MB: 20,
	DEFAULT_CHUNK_SIZE: 512 * 1024,

	// --- ãƒ–ãƒ­ãƒƒã‚¯å……å¡«ç‡ & ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š ---
	EFFECTIVE_BLOCK_SIZE_RATIO: 0.5, // 1ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Šã®ç›®æ¨™å……å¡«ç‡ (ä¾‹: 0.25 = 25%) â˜… ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ã§ 0.5 ã‹ã‚‰ç¸®å°
	PIPELINE_MAX_PENDING_BATCHES: 1,  // åŒæ™‚ã«å®Œäº†ã‚’å¾…ã¤ãƒãƒƒãƒã®æœ€å¤§æ•°

	// --- Mempool ç›£è¦–è¨­å®š (ãƒã‚¤ãƒˆã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹) ---
	MEMPOOL_BYTES_LIMIT: 5 * 1024 * 1024, // Mempoolã®åˆè¨ˆãƒã‚¤ãƒˆã‚µã‚¤ã‚ºä¸Šé™ (ä¾‹: 5MB)
	MEMPOOL_CHECK_INTERVAL_MS: 5000,     // Mempoolãƒã‚§ãƒƒã‚¯é–“éš” (ãƒŸãƒªç§’)

	// --- ãã®ä»–è¨­å®š ---
	TX_OVERHEAD_RATIO: 1.1,             // TXã‚µã‚¤ã‚ºã®è¦‹ç©ã‚‚ã‚Šç”¨ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ä¿‚æ•°
	RECONNECT_DELAY_MS: 3000,           // WebSocketå†æ¥ç¶šè©¦è¡Œé–“éš”
	WEBSOCKET_CONNECT_TIMEOUT_MS: 5000, // WebSocketæ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
	GAS_PRICE_STRING: '0.0000001uatom', // ã‚¬ã‚¹ä¾¡æ ¼
	GAS_MULTIPLIER: 1.5,                // ã‚¬ã‚¹è¦‹ç©ã‚‚ã‚Šã«å¯¾ã™ã‚‹ä¹—æ•°
	HD_PATH: "m/44'/118'/0'/0/2",        // HDã‚¦ã‚©ãƒ¬ãƒƒãƒˆãƒ‘ã‚¹
	RETRY_BACKOFF_MS: 500,              // ãƒªãƒˆãƒ©ã‚¤æ™‚ã®åŸºæœ¬å¾…æ©Ÿæ™‚é–“
	DEFAULT_TEST_SIZE_KB: 100 * 1024,   // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾ŒKB)
	TX_EVENT_TIMEOUT_MS: 120000,        // Txã‚¤ãƒ™ãƒ³ãƒˆã®å¾…æ©Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (ãƒŸãƒªç§’)
};

// å‹å®šç¾©
interface TransformableInfo extends winston.Logform.TransformableInfo { level: string; message: string;[key: string]: any; }
interface ChainInfo { name: string; type: 'datachain' | 'metachain'; }
interface ChainEndpoints { [key: string]: string; }
interface ExtendedChainClients { client: SigningStargateClient; account: AccountData; tmClient: Comet38Client; wsClient: WebsocketClient; restEndpoint: string; }
interface MegaChunkJob { buffer: Buffer; indexPrefix: string; chainName: string; retries: number; }

// ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒƒãƒ•ã‚¡å‹å®šç¾©ã¨ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
interface MsgCreateStoredChunk { creator: string; index: string; data: Uint8Array; }
const MsgCreateStoredChunkProto = {
	create(base?: Partial<MsgCreateStoredChunk>): MsgCreateStoredChunk { return { creator: base?.creator ?? "", index: base?.index ?? "", data: base?.data ?? new Uint8Array(), }; },
	encode(message: MsgCreateStoredChunk, writer: Writer = Writer.create()): Writer {
		if (message.creator !== '') { writer.uint32(10).string(message.creator); }
		if (message.index !== '') { writer.uint32(18).string(message.index); }
		if (message.data.length !== 0) { writer.uint32(26).bytes(message.data); }
		return writer;
	},
	// decode ã¯å®Ÿéš›ã«ã¯ä½¿ã‚ãªã„ã®ã§ç°¡ç•¥åŒ–
	decode(input: Reader | Uint8Array, length?: number): MsgCreateStoredChunk { const reader = input instanceof Reader ? input : new Reader(input); return { creator: '', index: '', data: new Uint8Array() }; },
};
interface MsgCreateStoredManifest { creator: string; url: string; manifest: string; }
const MsgCreateStoredManifestProto = {
	create(base?: Partial<MsgCreateStoredManifest>): MsgCreateStoredManifest { return { creator: base?.creator ?? "", url: base?.url ?? "", manifest: base?.manifest ?? "", }; },
	encode(message: MsgCreateStoredManifest, writer: Writer = Writer.create()): Writer {
		if (message.creator !== "") { writer.uint32(10).string(message.creator); }
		if (message.url !== "") { writer.uint32(18).string(message.url); }
		if (message.manifest !== "") { writer.uint32(26).string(message.manifest); }
		return writer;
	},
	// decode ã¯å®Ÿéš›ã«ã¯ä½¿ã‚ãªã„ã®ã§ç°¡ç•¥åŒ–
	decode(input: Reader | Uint8Array, length?: number): MsgCreateStoredManifest { const reader = input instanceof Reader ? input : new Reader(input); return { creator: "", url: "", manifest: "" }; }
};
const customRegistry = new Registry([
	['/datachain.datastore.v1.MsgCreateStoredChunk', MsgCreateStoredChunkProto as GeneratedType],
	['/metachain.metastore.v1.MsgCreateStoredManifest', MsgCreateStoredManifestProto as GeneratedType],
]);

// =================================================================================================
// ğŸ“ II. LOGGER UTILITIES (CLASS-BASED) - å¤‰æ›´ãªã— (ãƒ­ã‚°åˆ†é›¢ ä¿®æ­£æ¸ˆã¿)
// =================================================================================================

class LoggerUtil {
	private readonly logBuffer: TransformableInfo[] = [];
	private readonly logger: winston.Logger;
	private readonly logFilePath: string; // â˜… ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
	private readonly allLogFilePath: string; // â˜… å…¨ãƒ­ã‚°ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

	constructor() {
		const scriptFileName = path.basename(process.argv[1]!).replace(path.extname(process.argv[1]!), '');
		// ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ç¢ºèªãƒ»ä½œæˆ
		const logDir = path.join(process.cwd(), "src/tests/");
		try {
			if (!fs.existsSync(logDir)) {
				fs.mkdirSync(logDir, { recursive: true });
			}
		} catch (e) {
			console.error(`Error creating log directory ${logDir}:`, e);
		}
		this.logFilePath = path.join(logDir, `${scriptFileName}.error.log`); // â˜… ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ãƒ‘ã‚¹
		this.allLogFilePath = path.join(logDir, `${scriptFileName}.all.log`); // â˜… å…¨ãƒ­ã‚°ã®ãƒ‘ã‚¹

		class LogBufferTransport extends Transport {
			private readonly buffer: TransformableInfo[];
			constructor(buffer: TransformableInfo[], opts?: Transport.TransportStreamOptions) {
				super(opts);
				this.buffer = buffer;
			}
			log(info: any, callback: () => void) {
				setImmediate(() => { this.emit('logged', info); });
				this.buffer.push(info);
				callback();
			}
		}

		this.logger = winston.createLogger({
			level: 'debug', // â˜… ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ debug ãƒ¬ãƒ™ãƒ«ã‹ã‚‰æ›¸ãè¾¼ã‚€
			format: winston.format.combine(
				winston.format.timestamp({ format: 'YYYY-MM-DDTHH:mm:ss.SSSZ' }),
				winston.format.printf(info => `[${info.timestamp}] [${info.level.toUpperCase()}] - ${info.message} ${info.stack ? '\n' + info.stack : ''}`)
			),
			transports: [
				// 1. ãƒãƒƒãƒ•ã‚¡ (æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ç”¨)
				new LogBufferTransport(this.logBuffer),
				// 2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
				new winston.transports.File({
					filename: this.allLogFilePath,
					format: winston.format.combine(
						winston.format.timestamp({ format: 'YYYY-MM-DDTHH:mm:ss.SSSZ' }),
						winston.format.printf(info => `[${info.timestamp}] [${info.level.toUpperCase()}] - ${info.message} ${info.stack ? '\n' + info.stack : ''}`)
					),
					level: 'debug', // debugãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®å…¨ãƒ­ã‚°ã‚’æ›¸ãè¾¼ã‚€
					options: { flags: 'w' } // å®Ÿè¡Œã®ãŸã³ã«ä¸Šæ›¸ã
				})
				// 3. ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆã¯å‰Šé™¤ (ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã®ç«¶åˆå›é¿)
			],
		});

		// èµ·å‹•æ™‚ã«å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã«é€šçŸ¥
		console.error(`[LOGGER] ãƒ­ã‚°ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã•ã‚Œã¾ã™: ${this.allLogFilePath}`);
	}

	public getLogger(): winston.Logger {
		return this.logger;
	}

	public async flushLogs() {
		if (this.logBuffer.length === 0) return;
		// ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨ç¢ºèªã‚’è¿½åŠ 
		const logDir = path.dirname(this.logFilePath);
		try {
			fs.mkdirSync(logDir, { recursive: true });
		} catch (e) {
			console.error(`Error ensuring log directory ${logDir} exists:`, e);
		}
		const logContent = this.logBuffer
			.map(info => {
				const transformed = this.logger.format.transform(info, {});
				// ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ãŒinfoã§ãªã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ (ã‚¨ãƒ©ãƒ¼ç­‰)
				return transformed && (transformed as any).message && info.level !== 'info' ? (transformed as any).message : '';
			})
			.filter(line => line.length > 0)
			.join('\n');

		// â˜… ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿
		if (logContent.length > 0) { // ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã®ã¿æ›¸ãè¾¼ã‚€
			try {
				// this.logFilePath ã¯ .error.log ã«ãªã£ã¦ã„ã‚‹
				fs.writeFileSync(this.logFilePath, logContent + '\n', { flag: 'w' });
				// æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã«ã€ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’é€šçŸ¥
				console.error(`\nğŸš¨ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ã¾ã—ãŸ: ${this.logFilePath}`);
			} catch (e) {
				console.error('ERROR: Failed to write error logs to file.', e);
			}
		}
	}
}

const loggerUtil = new LoggerUtil();
const logger = loggerUtil.getLogger();

// =================================================================================================
// ğŸ’» III. KUBERNETES UTILITIES - å¤‰æ›´ãªã—
// =================================================================================================

const kc = new k8s.KubeConfig();
kc.loadFromDefault();
const k8sApi = kc.makeApiClient(k8s.CoreV1Api);

/**
 * Kubernetesã‹ã‚‰ãƒã‚§ãƒ¼ãƒ³æƒ…å ±ã¨REST/RPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹
 */
async function getChainResources(): Promise<{ chains: ChainInfo[], rpcEndpoints: ChainEndpoints, restEndpoints: ChainEndpoints }> {
	const resPods = await k8sApi.listNamespacedPod({
		namespace: CONFIG.K8S_NAMESPACE,
		labelSelector: 'app.kubernetes.io/component in (datachain, metachain)',
	});
	// Podãƒ©ãƒ™ãƒ«ã‹ã‚‰ãƒã‚§ãƒ¼ãƒ³åã¨ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
	const chains: ChainInfo[] = resPods.items.map(pod => ({
		name: pod.metadata!.labels!['app.kubernetes.io/instance']!,
		type: pod.metadata!.labels!['app.kubernetes.io/component']! as any,
	}));
	const rpcEndpoints: ChainEndpoints = {};
	const restEndpoints: ChainEndpoints = {};
	const isLocal = process.env.NODE_ENV !== 'production'; // ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã©ã†ã‹ã®åˆ¤å®š

	// Serviceæƒ…å ±ã‚’å–å¾—ã—ã¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æ§‹ç¯‰
	const resServices = await k8sApi.listNamespacedService({
		namespace: CONFIG.K8S_NAMESPACE,
		labelSelector: "app.kubernetes.io/category=chain" // ãƒã‚§ãƒ¼ãƒ³é–¢é€£ã®Serviceã‚’çµã‚Šè¾¼ã¿
	});

	for (const chain of chains) {
		const serviceName = `raidchain-${chain.name}-headless`; // Headless Serviceå
		const service = resServices.items.find(s => s.metadata?.name === serviceName);
		if (isLocal) { // ãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆ (NodePortã‚’æƒ³å®š)
			const rpcPortInfo = service?.spec?.ports?.find(p => p.name === 'rpc');
			const apiPortInfo = service?.spec?.ports?.find(p => p.name === 'api');
			if (rpcPortInfo?.nodePort) { rpcEndpoints[chain.name] = `http://localhost:${rpcPortInfo.nodePort}`; }
			if (apiPortInfo?.nodePort) { restEndpoints[chain.name] = `http://localhost:${apiPortInfo.nodePort}`; }
		} else { // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…éƒ¨ã®å ´åˆ (ClusterIP/Headless Service FQDN)
			const podHostName = `raidchain-${chain.name}-0`; // StatefulSetã®Podå (ä¾‹: raidchain-data-0-0)
			const headlessServiceName = `raidchain-chain-headless`; // values.yamlç­‰ã§å®šç¾©ã•ã‚ŒãŸå…±é€šã®Headless Serviceå
			rpcEndpoints[chain.name] = `http://${podHostName}.${headlessServiceName}.${CONFIG.K8S_NAMESPACE}.svc.cluster.local:26657`;
			restEndpoints[chain.name] = `http://${podHostName}.${headlessServiceName}.${CONFIG.K8S_NAMESPACE}.svc.cluster.local:1317`;
		}
		// ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
		if (!rpcEndpoints[chain.name]) logger.warn(`RPC endpoint not found for ${chain.name}`);
		if (!restEndpoints[chain.name]) logger.warn(`REST endpoint not found for ${chain.name}`);
	}
	return { chains, rpcEndpoints, restEndpoints };
}

/**
 * Kubernetes Secretã‹ã‚‰ãƒ‹ãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ã‚’å–å¾—ã™ã‚‹
 */
async function getCreatorMnemonic(chainName: string): Promise<string> {
	const res = await k8sApi.readNamespacedSecret({
		name: CONFIG.SECRET_NAME,
		namespace: CONFIG.K8S_NAMESPACE,
	});
	const encodedMnemonic = res.data?.[`${chainName}.mnemonic`]; // Secretå†…ã®ã‚­ãƒ¼ (ä¾‹: data-0.mnemonic)
	if (!encodedMnemonic) throw new Error(`Secret ${CONFIG.SECRET_NAME} does not contain mnemonic for ${chainName}.`);
	// Base64ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¿”ã™
	return Buffer.from(encodedMnemonic, 'base64').toString('utf-8');
}

// =================================================================================================
// ğŸš€ IV. CHAIN CLIENT & TRANSACTION MANAGEMENT (â˜… å¤§å¹…ä¿®æ­£)
// =================================================================================================

// ---------------------------------------------------------------------------------
// â˜… æ–°è¨­: TxEventSubscriber ã‚¯ãƒ©ã‚¹
// ãƒ¯ãƒ¼ã‚«ãƒ¼ã”ã¨ã«1ã¤ã®è³¼èª­ã‚’ç¶­æŒã—ã€ãƒãƒƒãƒå¾…æ©Ÿã‚’ç®¡ç†ã™ã‚‹
// ---------------------------------------------------------------------------------

/**
 * 1ã¤ã®WebSocketè³¼èª­ã‚’ç¶­æŒã—ã€è¤‡æ•°ã®ãƒãƒƒãƒå¾…æ©Ÿå‡¦ç†ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
 */
class TxEventSubscriber {
	private readonly tmClient: Comet38Client;
	private readonly bar: cliProgress.SingleBar;
	private readonly chainName: string; // ãƒ­ã‚°ç”¨

	private stream: Stream<TxEvent> | null = null;
	private listener: Listener<TxEvent> | null = null;
	private isSubscribed = false;

	// å¾…æ©Ÿä¸­ã®ã‚¸ãƒ§ãƒ–ã‚’ç®¡ç†ã™ã‚‹ (Key: TxHash (Uppercase))
	private pendingJobs = new Map<string, {
		jobInfo: BatchWaitJob;
	}>();

	// å¾…æ©Ÿä¸­ã®ãƒãƒƒãƒã‚¸ãƒ§ãƒ– (Promise) ã‚’ç®¡ç†ã™ã‚‹
	private activeJobs = new Set<BatchWaitJob>();

	constructor(tmClient: Comet38Client, bar: cliProgress.SingleBar, chainName: string) {
		this.tmClient = tmClient;
		this.bar = bar;
		this.chainName = chainName;
	}

	/**
	 * ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®è³¼èª­ã‚’é–‹å§‹ã—ã€å…±æœ‰ãƒªã‚¹ãƒŠãƒ¼ã‚’ã‚¢ã‚¿ãƒƒãƒã™ã‚‹
	 */
	public async start(): Promise<void> {
		if (this.isSubscribed) return;

		const query = `tm.event = 'Tx'`;
		this.stream = this.tmClient.subscribeTx(query) as Stream<TxEvent>;

		this.listener = {
			next: (event: TxEvent) => {
				this.onEvent(event);
			},
			error: (err: any) => {
				logger.error(`[${this.chainName}] [EVENT_ERROR] Critical error in Tx subscription stream:`, err);
				// ã™ã¹ã¦ã®å¾…æ©Ÿä¸­ã‚¸ãƒ§ãƒ–ã‚’ã‚¨ãƒ©ãƒ¼ã§å¼·åˆ¶çµ‚äº†ã•ã›ã‚‹
				this.activeJobs.forEach(job => {
					job.masterReject(new Error(`Tx subscription stream error: ${err.message}`));
				});
				this.cleanup(); // ãƒªã‚¹ãƒŠãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
			},
			complete: () => {
				logger.warn(`[${this.chainName}] [EVENT_COMPLETE] Tx subscription stream completed unexpectedly.`);
				// ãƒªã‚¹ãƒŠãƒ¼ãŒæ­¢ã¾ã£ãŸã®ã§ã€é–‹ã„ã¦ã„ã‚‹ã‚¸ãƒ§ãƒ–ãŒã‚ã‚Œã°ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å¾…ãŸãšã«çµ‚äº†ã•ã›ã‚‹
				this.activeJobs.forEach(job => {
					job.masterResolve(job.confirmationStatus); // ç¾åœ¨ã®çŠ¶æ…‹ã§å®Œäº†ã•ã›ã‚‹
				});
				this.cleanup();
			},
		};

		this.stream.addListener(this.listener);
		this.isSubscribed = true;
		logger.debug(`[${this.chainName}] [EVENT_SUB] Subscribed to all Tx events.`);
	}

	/**
	 * è³¼èª­ã‚’åœæ­¢ã—ã€ãƒªã‚¹ãƒŠãƒ¼ã‚’è§£é™¤ã™ã‚‹
	 */
	public stop(): void {
		this.cleanup();
	}

	/**
	 * å†…éƒ¨ãƒªã‚¹ãƒŠãƒ¼: ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†ã—ã€è©²å½“ã™ã‚‹å¾…æ©Ÿã‚¸ãƒ§ãƒ–ã«æŒ¯ã‚Šåˆ†ã‘ã‚‹
	 */
	private onEvent(event: TxEvent): void {
		const receivedHash = Buffer.from(event.hash).toString("hex").toUpperCase();

		const pending = this.pendingJobs.get(receivedHash);

		// è©²å½“ã™ã‚‹å¾…æ©Ÿã‚¸ãƒ§ãƒ–ãŒãªã‘ã‚Œã°ç„¡è¦–
		if (!pending) {
			// logger.debug(`[${this.chainName}] [EVENT_RECV] Received event for unknown hash: ${receivedHash.substring(0, 10)}...`);
			return;
		}

		const job = pending.jobInfo;

		// ã™ã§ã«ç¢ºèªæ¸ˆã¿ã®å ´åˆã¯é‡è¤‡ãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ï¼‰
		if (job.confirmationStatus.get(receivedHash)?.height) {
			logger.debug(`[${this.chainName}] [EVENT_RECV] Received duplicate confirmation for Tx ${receivedHash.substring(0, 10)}...`);
			return;
		}

		// ---------------------------------
		// è©²å½“ã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
		// ---------------------------------
		const success = event.result.code === 0;
		const height = event.height;
		logger.debug(`[${this.chainName}] [EVENT_RECV] Tx ${receivedHash.substring(0, 10)}... confirmed in block ${height}. Success: ${success}`);

		// 1. ãƒãƒƒãƒã®çµæœMapã‚’æ›´æ–°
		job.confirmationStatus.set(receivedHash, { success, height });
		// 2. ã“ã®ãƒãƒƒãƒã§ç¢ºèªæ¸ˆã¿ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’å¢—ã‚„ã™
		job.confirmedCountInBatch++;
		// 3. å¾…æ©Ÿä¸­ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤
		this.pendingJobs.delete(receivedHash);

		// 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
		const totalCompleted = job.completedTxOffset + job.confirmedCountInBatch;
		const elapsedMs = Date.now() - job.batchStartTime;
		const txPerSec = (job.confirmedCountInBatch * 1000 / Math.max(elapsedMs, 1)).toFixed(2);
		job.bar.update(totalCompleted, {
			height: height,
			tx_per_sec: txPerSec,
			status: `Confirming (${job.confirmedCountInBatch}/${job.totalTxInBatch})`
		});

		// 5. ã“ã®ãƒãƒƒãƒãŒå®Œäº†ã—ãŸã‹ãƒã‚§ãƒƒã‚¯
		if (job.confirmedCountInBatch === job.expectedConfirmations) {
			logger.info(`[${this.chainName}] [EVENT_WAIT] All ${job.confirmedCountInBatch} expected transactions confirmed for this batch.`);
			job.cleanupTimeout(); // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒªã‚¢
			this.activeJobs.delete(job); // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¸ãƒ§ãƒ–ã‹ã‚‰å‰Šé™¤
			job.masterResolve(job.confirmationStatus); // ã“ã®ãƒãƒƒãƒã®Promiseã‚’è§£æ±º
		}
	}

	/**
	 * ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
	 */
	private cleanup(): void {
		if (this.stream && this.listener) {
			try {
				this.stream.removeListener(this.listener);
				logger.debug(`[${this.chainName}] [EVENT_CLEANUP] Cleaned up the listener.`);
			} catch (e) {
				logger.warn(`[${this.chainName}] Error removing listener (ignoring):`, e);
			}
		}
		this.stream = null;
		this.listener = null;
		this.isSubscribed = false;
		// å¿µã®ãŸã‚ä¿ç•™ä¸­ã®ã‚¸ãƒ§ãƒ–ã‚‚ã‚¯ãƒªã‚¢
		this.pendingJobs.clear();
		this.activeJobs.clear();
	}

	/**
	 * æŒ‡å®šã•ã‚ŒãŸTxãƒãƒƒã‚·ãƒ¥ãƒªã‚¹ãƒˆã®å®Œäº†ã‚’å¾…æ©Ÿã™ã‚‹ (Promiseã‚’è¿”ã™)
	 * (æ—§ waitForTxInclusionWithEvents ã®å½¹å‰²)
	 */
	public waitForTxs(
		targetHashes: string[],
		completedTxOffset: number,
		totalTxInBatch: number // ãƒãƒƒãƒå†…ã®ç·Txæ•°
	): Promise<Map<string, { success: boolean; height: number | undefined }>> {

		// ã“ã®ãƒãƒƒãƒå¾…æ©Ÿã‚¸ãƒ§ãƒ–ã®å…¨ä½“ã‚’ç®¡ç†ã™ã‚‹Promise
		return new Promise((resolve, reject) => {

			const confirmationStatus = new Map<string, { success: boolean; height: number | undefined }>();
			const targetHashSet = new Set<string>();

			targetHashes.forEach(hash => {
				if (hash.startsWith("ERROR_BROADCASTING")) {
					confirmationStatus.set(hash, { success: false, height: undefined });
				} else {
					confirmationStatus.set(hash, { success: false, height: undefined }); // åˆæœŸå€¤
					targetHashSet.add(hash.toUpperCase());
				}
			});

			const expectedConfirmations = targetHashSet.size;

			// ç›£è¦–å¯¾è±¡ãŒ0ï¼ˆãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤±æ•—ã®ã¿ï¼‰ãªã‚‰å³å®Œäº†
			if (expectedConfirmations === 0) {
				logger.info(`[${this.chainName}] [EVENT_WAIT] No valid transactions to wait for in this batch.`);
				resolve(confirmationStatus);
				return;
			}

			let timeoutId: NodeJS.Timeout | null = null;

			// ãƒãƒƒãƒå¾…æ©Ÿã‚¸ãƒ§ãƒ–ã®æƒ…å ±ã‚’ç”Ÿæˆ
			const job: BatchWaitJob = {
				confirmationStatus,
				expectedConfirmations,
				confirmedCountInBatch: 0,
				totalTxInBatch,
				batchStartTime: Date.now(),
				bar: this.bar,
				completedTxOffset,
				masterResolve: resolve,
				masterReject: reject,
				cleanupTimeout: () => {
					if (timeoutId) clearTimeout(timeoutId);
				}
			};

			// ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†
			timeoutId = setTimeout(() => {
				const unconfirmed = Array.from(confirmationStatus.entries())
					.filter(([hash, status]) => targetHashSet.has(hash.toUpperCase()) && !status.height)
					.map(([hash, _]) => hash.substring(0, 10) + "...");
				logger.error(`[${this.chainName}] [EVENT_WAIT] Timeout (${CONFIG.TX_EVENT_TIMEOUT_MS / 1000}s) waiting for Tx events. ${job.confirmedCountInBatch}/${expectedConfirmations} confirmed.`);
				logger.error(` Unconfirmed: ${unconfirmed.join(', ')}`);

				// ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ã€ä¿ç•™ä¸­ã®ãƒãƒƒã‚·ãƒ¥ã‚’ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤
				targetHashSet.forEach(hash => {
					if (!confirmationStatus.get(hash)?.height) {
						this.pendingJobs.delete(hash);
					}
				});

				this.activeJobs.delete(job); // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¸ãƒ§ãƒ–ã‹ã‚‰å‰Šé™¤
				resolve(confirmationStatus); // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ Map ã‚’ãã®ã¾ã¾è¿”ã™
			}, CONFIG.TX_EVENT_TIMEOUT_MS);

			// ã“ã®ã‚¸ãƒ§ãƒ–ã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚¹ãƒˆã«è¿½åŠ 
			this.activeJobs.add(job);

			// å¾…æ©Ÿå¯¾è±¡ã®å…¨ãƒãƒƒã‚·ãƒ¥ã‚’ã€å…±æœ‰ã® pendingJobs Map ã«ç™»éŒ²
			targetHashSet.forEach(hash => {
				this.pendingJobs.set(hash, { jobInfo: job });
			});

			logger.debug(`[${this.chainName}] [EVENT_WAIT] Waiting for ${expectedConfirmations} TXs for this batch...`);

		}); // return new Promise
	}
}

/**
 * TxEventSubscriber å†…éƒ¨ã§ãƒãƒƒãƒå¾…æ©Ÿæƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®å‹
 */
interface BatchWaitJob {
	confirmationStatus: Map<string, { success: boolean; height: number | undefined }>;
	expectedConfirmations: number;
	confirmedCountInBatch: number;
	totalTxInBatch: number;
	batchStartTime: number;
	bar: cliProgress.SingleBar;
	completedTxOffset: number;
	masterResolve: (value: Map<string, { success: boolean; height: number | undefined }>) => void;
	masterReject: (reason?: any) => void;
	cleanupTimeout: () => void;
}


// ---------------------------------------------------------------------------------
// ChainManager ã‚¯ãƒ©ã‚¹ (â˜… waitForBatchInclusionWithEvents ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‰Šé™¤)
// ---------------------------------------------------------------------------------

/**
 * Cosmos SDKãƒã‚§ãƒ¼ãƒ³ã¨ã®ã‚„ã‚Šå–ã‚Šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
 */
class ChainManager {
	private readonly chainClients = new Map<string, ExtendedChainClients>();
	public readonly gasPrice: GasPrice;

	private allChains: ChainInfo[] = [];
	private rpcEndpoints: ChainEndpoints = {};
	private restEndpoints: ChainEndpoints = {};

	constructor() {
		this.gasPrice = GasPrice.fromString(CONFIG.GAS_PRICE_STRING);
	}

	/**
	 * å†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ (k8sãƒªã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨) - å¤‰æ›´ãªã—
	 */
	private async setupSingleClient(chain: ChainInfo, rpcEndpoints: ChainEndpoints, restEndpoints: ChainEndpoints): Promise<void> {
		const chainName = chain.name;
		try {
			const mnemonic = await getCreatorMnemonic(chainName);
			const wallet = await DirectSecp256k1HdWallet.fromMnemonic(mnemonic, { hdPaths: [stringToPath(CONFIG.HD_PATH)] });
			const [account] = await wallet.getAccounts();
			if (!account) throw new Error(`Failed to get account from wallet for chain ${chainName}`);

			const rpcUrl = rpcEndpoints[chainName]!.replace('http', 'ws'); // WebSocket URL ã«å¤‰æ›
			const wsClient = new WebsocketClient(rpcUrl, (err) => { // ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ã‚’è¨­å®š
				if (err) { logger.warn(`[${chainName}] WebSocket connection error: ${err.message}. Will attempt reconnect on next operation.`); }
				// å¿…è¦ã§ã‚ã‚Œã°ã“ã“ã§å†æ¥ç¶šå‡¦ç†ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½
			});

			// æ¥ç¶šç¢ºèª: `execute` ã‚’ä½¿ã£ã¦ status ã‚’å–å¾—ã—ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
			const connectPromise = wsClient.execute({ jsonrpc: "2.0", method: "status", id: `connect-${Date.now()}`, params: {} });
			const timeoutPromise = new Promise((_, reject) => setTimeout(() => reject(new Error("WebSocket connection timed out")), CONFIG.WEBSOCKET_CONNECT_TIMEOUT_MS));
			await Promise.race([connectPromise, timeoutPromise]);
			logger.debug(`[${chainName}] WebSocket connected via status check.`);

			// Tendermint37Client ã‚’ WebSocketClient ã‹ã‚‰ä½œæˆ
			const tmClient = Comet38Client.create(wsClient);
			// SigningStargateClient ã‚’ Tendermint37Client ã‹ã‚‰ä½œæˆ
			const client = SigningStargateClient.createWithSigner(tmClient, wallet, { registry: customRegistry, gasPrice: this.gasPrice });

			// ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’ Map ã«ä¿å­˜
			this.chainClients.set(chainName, { client, account, tmClient, wsClient, restEndpoint: restEndpoints[chainName]! });
			logger.info(`[CLIENT_SETUP] Successful for chain: ${chainName} (Address: ${account.address})`);
		} catch (e) {
			logger.error(`[CLIENT_SETUP] Failed to initialize client for chain ${chainName}:`, e);
			// å¤±æ•—ã—ãŸå ´åˆã€wsClient ãŒå­˜åœ¨ã™ã‚Œã°åˆ‡æ–­ã‚’è©¦ã¿ã‚‹
			const existingClient = this.chainClients.get(chainName);
			if (existingClient?.wsClient) {
				try { existingClient.wsClient.disconnect(); } catch { }
			}
			this.chainClients.delete(chainName); // å¤±æ•—ã—ãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã¯å‰Šé™¤
			throw e; // ã‚¨ãƒ©ãƒ¼ã‚’å†ã‚¹ãƒ­ãƒ¼
		}
	}

	/**
	 * ã™ã¹ã¦ã®ãƒã‚§ãƒ¼ãƒ³ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹ - å¤‰æ›´ãªã—
	 */
	public async initializeClients(allChains: ChainInfo[], rpcEndpoints: ChainEndpoints, restEndpoints: ChainEndpoints): Promise<void> {
		this.allChains = allChains;
		this.rpcEndpoints = rpcEndpoints;
		this.restEndpoints = restEndpoints;

		// å„ãƒã‚§ãƒ¼ãƒ³ã«å¯¾ã—ã¦ setupSingleClient ã‚’ä¸¦åˆ—å®Ÿè¡Œ
		const initPromises = allChains.map(chain =>
			this.setupSingleClient(chain, rpcEndpoints, restEndpoints)
				.catch(e => logger.error(`[INIT_FAIL] Skipping client for ${chain.name} due to error.`)) // å€‹åˆ¥ã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°å‡ºåŠ›ã®ã¿
		);
		await Promise.allSettled(initPromises); // å…¨ã¦ã®åˆæœŸåŒ–è©¦è¡Œå®Œäº†ã‚’å¾…ã¤

		// å°‘ãªãã¨ã‚‚1ã¤ã® datachain ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
		const dataChainNames = allChains.filter(c => c.type === 'datachain').map(c => c.name);
		const initializedDataChains = dataChainNames.filter(name => this.chainClients.has(name));
		if (initializedDataChains.length === 0 && dataChainNames.length > 0) {
			throw new Error("Failed to initialize any datachain clients.");
		}
		logger.info(`[INIT_COMPLETE] Initialized clients for: ${Array.from(this.chainClients.keys()).join(', ')}`);
	}

	/**
	 * æŒ‡å®šã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å†æ¥ç¶šã™ã‚‹ (k8sãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ä½¿ç”¨) - å¤‰æ›´ãªã—
	 */
	public async reconnectClient(chainName: string): Promise<void> {
		logger.warn(`[${chainName}] Attempting to reconnect client...`);

		// å¤ã„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã®å–å¾—ã¨åˆ‡æ–­
		const oldClientInfo = this.chainClients.get(chainName);
		if (oldClientInfo) {
			try {
				oldClientInfo.wsClient.disconnect();
				// Tendermint37Client ã«ã¯ disconnect ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ try-catch
				try { (oldClientInfo.tmClient as any)?.disconnect(); } catch { }
			} catch (e) {
				logger.warn(`[${chainName}] Error during old client disconnection (ignoring):`, e);
			}
		}
		this.chainClients.delete(chainName); // å¤ã„æƒ…å ±ã‚’å‰Šé™¤

		// ãƒã‚§ãƒ¼ãƒ³æƒ…å ±ã‚’å–å¾—
		const chainInfo = this.allChains.find(c => c.name === chainName);
		if (!chainInfo) {
			throw new Error(`[${chainName}] Cannot reconnect: ChainInfo not found.`);
		}

		// å†åº¦ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
		await this.setupSingleClient(chainInfo, this.rpcEndpoints, this.restEndpoints);
		logger.info(`[${chainName}] Reconnection attempt finished.`);
	}

	// getClientInfo (å¤‰æ›´ãªã—)
	public getClientInfo(chainName: string): ExtendedChainClients {
		const clientInfo = this.chainClients.get(chainName);
		if (!clientInfo) throw new Error(`Client not initialized for chain: ${chainName}`);
		return clientInfo;
	}

	// Mempool ãƒã‚¤ãƒˆã‚µã‚¤ã‚ºå–å¾—ãƒ¡ã‚½ãƒƒãƒ‰ (å¤‰æ›´ãªã—)
	/**
	 * Mempoolã®æœªç¢ºèªTxåˆè¨ˆãƒã‚¤ãƒˆã‚µã‚¤ã‚ºã‚’å–å¾—ã™ã‚‹
	 */
	public async getMempoolTotalBytes(chainName: string): Promise<number> {
		const { tmClient } = this.getClientInfo(chainName);
		try {
			// Tendermint37Client ã® numUnconfirmedTxs ã¯ totalBytes ã‚’å«ã‚€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
			const result = await tmClient.numUnconfirmedTxs();
			const bytes = Number(result.totalBytes); // total_bytes ã‚’æ•°å€¤ã«å¤‰æ›
			return isNaN(bytes) ? 0 : bytes;
		} catch (error) {
			logger.warn(`[${chainName}] Failed to get mempool total_bytes:`, error);
			throw error; // ã‚¨ãƒ©ãƒ¼ã‚’å†ã‚¹ãƒ­ãƒ¼ã—ã¦ waitForMempoolSpace ã§å‡¦ç†ã•ã›ã‚‹
		}
	}

	// â˜… å‰Šé™¤: waitForBatchInclusionWithEvents ãƒ¡ã‚½ãƒƒãƒ‰
	// (TxEventSubscriber ã‚¯ãƒ©ã‚¹ãŒã“ã®å½¹å‰²ã‚’æ‹…ã†ãŸã‚ä¸è¦ã«ãªã£ãŸ)


	/**
	 * é€ä¿¡å°‚ç”¨ã®é–¢æ•° - å¤‰æ›´ãªã—
	 */
	public async broadcastSequentialTxs(
		chainName: string,
		messages: EncodeObject[],
		estimatedGas: number,
		bar: cliProgress.SingleBar,
		completedTxOffset: number = 0, // ãƒãƒ¼è¡¨ç¤ºã®ãŸã‚ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ
		currentSequenceRef: { sequence: number } // å¤–éƒ¨ã§ç®¡ç†ã•ã‚Œã‚‹ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ç•ªå·
	): Promise<string[]> {
		const { client, account } = this.getClientInfo(chainName);
		const gasWanted = Math.round(estimatedGas * CONFIG.GAS_MULTIPLIER);
		const fee = calculateFee(gasWanted, this.gasPrice);
		const totalTxsInBatch = messages.length;

		// ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±å–å¾—ï¼ˆaccountNumberã®ãŸã‚ï¼‰
		// æ³¨æ„: ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ã¯å¤–éƒ¨ã® currentSequenceRef ã‚’ä½¿ç”¨
		const accountInfo = await client.getAccount(account.address);
		if (!accountInfo) throw new Error(`Failed to get account info for ${account.address} on ${chainName}`);
		const accountNumber = accountInfo.accountNumber;
		const chainId = await client.getChainId();

		const txHashes: string[] = [];
		logger.debug(`[${chainName}] Starting broadcast loop. Initial sequence: ${currentSequenceRef.sequence}`);

		for (let i = 0; i < totalTxsInBatch; i++) {
			const msg = messages[i]!;
			const sequence = currentSequenceRef.sequence; // ç¾åœ¨ã®ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ã‚’ä½¿ç”¨

			// ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç½²å
			const signedTx = await client.sign(
				account.address, [msg], fee,
				`Batch Tx ${i + 1}/${totalTxsInBatch} (Seq: ${sequence})`, // ãƒ¡ãƒ¢
				{ accountNumber, sequence, chainId }
			);
			const txRaw = Uint8Array.from(TxRaw.encode(signedTx).finish());

			try {
				// åŒæœŸãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ (Mempoolã«è¿½åŠ ã•ã‚Œã‚‹ã¾ã§å¾…ã¤)
				const resultHash = await client.broadcastTxSync(txRaw);
				txHashes.push(resultHash);
				currentSequenceRef.sequence++; // æˆåŠŸã—ãŸã‚‰ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
				logger.debug(` -> [${chainName}] Tx ${i + 1} sent. Hash: ${resultHash.substring(0, 10)}... (Seq: ${sequence})`);

				// ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–° (é€ä¿¡çŠ¶æ³ã‚’è¡¨ç¤º)
				bar.update(completedTxOffset, { status: `Broadcasting ${txHashes.length}/${totalTxsInBatch}` });

			} catch (error: any) {
				// ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤±æ•—
				logger.error(`[CRITICAL_FAIL] Tx (Seq ${sequence}) failed to broadcast on ${chainName}. Error:`, error);
				// å¤±æ•—ã—ãŸTxã®ãƒãƒƒã‚·ãƒ¥ã¨ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’ç¤ºã™æ–‡å­—åˆ—ã‚’å…¥ã‚Œã‚‹ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆå¾…æ©Ÿã§ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŸã‚ï¼‰
				txHashes.push(`ERROR_BROADCASTING_TX_${i + 1}_SEQ_${sequence}`);
				// â˜… é‡è¦: ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤±æ•—æ™‚ã¯ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã—ãªã„ï¼
				// æ¬¡ã®ãƒªãƒˆãƒ©ã‚¤æ™‚ã«åŒã˜ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ç•ªå·ãŒå†åˆ©ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
				// ï¼ˆãŸã ã—ã€ç¾åœ¨ã®ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã¯ã‚·ãƒ¼ã‚­ãƒ³ã‚¹å†å–å¾—ã‚’è¡Œã†ã®ã§ã€ã“ã“ã§ã®ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆæœ‰ç„¡ã¯å½±éŸ¿å°ï¼‰
				throw new Error(`Broadcast failure (Seq ${sequence}) on ${chainName}: ${error.message}`); // ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã‚’ãƒˆãƒªã‚¬ãƒ¼
			}
		}
		logger.debug(`[${chainName}] Finished broadcast loop. Final sequence: ${currentSequenceRef.sequence}`);
		return txHashes;
	}

	/**
	 * WebSocketã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã™ã¹ã¦åˆ‡æ–­ã™ã‚‹ - å¤‰æ›´ãªã—
	 */
	public closeAllConnections(): void {
		logger.info('[CLEANUP] Closing all WebSocket connections...');
		for (const [chainName, { wsClient, tmClient }] of this.chainClients.entries()) {
			try {
				wsClient.disconnect();
				// Tendermint37Client ã«ã¯ disconnect ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆãŒã‚ã‚‹
				try { (tmClient as any)?.disconnect(); } catch { }
				logger.debug(`[${chainName}] WebSocket connection closed.`);
			} catch (e) {
				logger.warn(`[${chainName}] Error closing connection (ignoring):`, e);
			}
		}
		this.chainClients.clear(); // Mapã‚’ã‚¯ãƒªã‚¢
	}
}

// =================================================================================================
// âš™ï¸ V. CORE BUSINESS LOGIC (MAIN) - â˜… ä¿®æ­£
// =================================================================================================

/**
 * Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œã®ç›®æ¨™ã‚µã‚¤ã‚ºã‹ã‚‰ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’è¨ˆç®— - å¤‰æ›´ãªã—
 */
function getOriginalSizeForBase64Target(targetEncodedSizeInBytes: number): number {
	// Base64 ã¯ 3 ãƒã‚¤ãƒˆã‚’ 4 ãƒã‚¤ãƒˆã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€ç´„ 3/4 ã«ãªã‚‹
	return Math.floor(targetEncodedSizeInBytes * 3 / 4);
}

/**
 * ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ (ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿) ã¾ãŸã¯å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ - å¤‰æ›´ãªã—
 */
async function setupEnvironment(chainManager: ChainManager): Promise<{
	filePath: string,
	fileBuffer: Buffer,
	fileSizeInBytes: number,
	dataChains: ChainInfo[],
	metaChain: ChainInfo | null,
	megaChunkSize: number
}> {
	// --- 1. å¼•æ•°å‡¦ç† ---
	const args = process.argv.slice(2);
	const sizeIndex = args.indexOf('--size-kb');
	let filePath: string;
	let fileBuffer: Buffer;
	let fileSizeInBytes: number; // ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º

	if (sizeIndex !== -1 && args[sizeIndex + 1]) {
		// (A) --size-kb æŒ‡å®š: ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
		const targetEncodedSizeKB = parseInt(args[sizeIndex + 1]!, 10);
		if (isNaN(targetEncodedSizeKB) || targetEncodedSizeKB <= 0) throw new Error(`Invalid --size-kb: ${args[sizeIndex + 1]}`);
		const targetEncodedSizeBytes = targetEncodedSizeKB * 1024;
		fileSizeInBytes = getOriginalSizeForBase64Target(targetEncodedSizeBytes); // ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚µã‚¤ã‚ºé€†ç®—
		filePath = `memory-buffer-${targetEncodedSizeKB}kb-encoded`;
		logger.info(`[SETUP] Generating dummy data (Original: ~${(fileSizeInBytes / 1024 / 1024).toFixed(2)} MB, Target Encoded: ${targetEncodedSizeKB} KB)...`);
		fileBuffer = Buffer.alloc(fileSizeInBytes, `Dummy data for ${filePath}.`);
	} else if (args[0]) {
		// (B) ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æŒ‡å®š: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
		filePath = args[0];
		try {
			const stats = fs.statSync(filePath);
			fileBuffer = fs.readFileSync(filePath);
			fileSizeInBytes = stats.size;
			const fileSizeMB = (fileSizeInBytes / 1024 / 1024).toFixed(2);
			logger.info(`[SETUP] Loaded file: ${filePath} (${fileSizeMB} MB)`);
			const estimatedEncodedSizeMB = (fileSizeInBytes * 4 / 3 / 1024 / 1024).toFixed(2);
			logger.info(`          (Estimated encoded upload size: ~${estimatedEncodedSizeMB} MB)`);
		} catch (e) { throw new Error(`Failed to read file ${filePath}: ${e}`); }
	} else {
		// (C) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
		const targetEncodedSizeKB = CONFIG.DEFAULT_TEST_SIZE_KB;
		const targetEncodedSizeBytes = targetEncodedSizeKB * 1024;
		fileSizeInBytes = getOriginalSizeForBase64Target(targetEncodedSizeBytes);
		filePath = `memory-buffer-${targetEncodedSizeKB}kb-encoded-default`;
		logger.info(`[SETUP] No input specified. Generating default dummy data (Original: ~${(fileSizeInBytes / 1024 / 1024).toFixed(2)} MB, Target Encoded: ${targetEncodedSizeKB} KB)...`);
		fileBuffer = Buffer.alloc(fileSizeInBytes, `Default dummy data.`);
	}

	// --- 2. ç’°å¢ƒæƒ…å ±å–å¾— (k8s) ---
	logger.info("[SETUP] Fetching Kubernetes resources...");
	const { chains: allChains, rpcEndpoints, restEndpoints } = await getChainResources();
	const dataChains = allChains.filter(c => c.type === 'datachain');
	const metaChain = allChains.find(c => c.type === 'metachain') || null;
	const numDataChains = dataChains.length;
	if (numDataChains === 0) throw new Error('No Datachains found in Kubernetes.');
	logger.info(`[SETUP] Found ${numDataChains} datachains: ${dataChains.map(c => c.name).join(', ')}`);
	if (metaChain) logger.info(`[SETUP] Found metachain: ${metaChain.name}`); else logger.warn('[SETUP] Metachain not found.');

	// --- 3. ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®— ---
	const megaChunkSize = Math.ceil(fileSizeInBytes / numDataChains);
	logger.info(`[SETUP] MegaChunk size per chain: ~${Math.round(megaChunkSize / 1024)} KB`);
	logger.info(`[SETUP] MiniChunk (TX) size: ${Math.round(CONFIG.DEFAULT_CHUNK_SIZE / 1024)} KB`);

	// --- 4. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
	logger.info("[SETUP] Initializing chain clients...");
	await chainManager.initializeClients(allChains, rpcEndpoints, restEndpoints);

	return { filePath, fileBuffer, fileSizeInBytes, dataChains, metaChain, megaChunkSize };
}

/**
 * ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€ãƒã‚§ãƒ¼ãƒ³ã”ã¨ã®ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ã«å‰²ã‚Šå½“ã¦ã‚‹ - å¤‰æ›´ãªã—
 */
async function createMegaChunkJobs(fileBuffer: Buffer, megaChunkSize: number, dataChains: ChainInfo[]): Promise<{ jobsByChain: Map<string, MegaChunkJob[]>, totalMegaChunks: number }> {
	const jobsByChain = new Map<string, MegaChunkJob[]>();
	dataChains.forEach(chain => jobsByChain.set(chain.name, []));
	let chunkCounter = 0;
	const uniqueSuffix = `dist-seq-test-${Date.now()}`;
	const numDataChains = dataChains.length;
	let offset = 0;

	logger.info(`[CHUNK_SPLIT] Splitting buffer (size: ${fileBuffer.length} B) into MegaChunks (size: ${megaChunkSize} B)...`);
	while (offset < fileBuffer.length) {
		const end = Math.min(offset + megaChunkSize, fileBuffer.length);
		const buffer = fileBuffer.slice(offset, end);
		const indexPrefix = `${uniqueSuffix}-mega-${chunkCounter}`; // å„ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã®ä¸€æ„ãªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
		const targetChainIndex = chunkCounter % numDataChains;
		const targetChainName = dataChains[targetChainIndex]!.name; // ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ã§å‰²ã‚Šå½“ã¦

		const job: MegaChunkJob = { buffer, indexPrefix, chainName: targetChainName, retries: 0 };
		jobsByChain.get(targetChainName)!.push(job);

		logger.debug(` -> MegaChunk ${chunkCounter}: ${buffer.length} B assigned to ${targetChainName}`);
		offset = end;
		chunkCounter++;
	}

	logger.info(`[CHUNK_SPLIT] Buffer split into ${chunkCounter} MegaChunks.`);
	dataChains.forEach(chain => {
		logger.info(`  -> Chain ${chain.name} assignment: ${jobsByChain.get(chain.name)!.length} MegaChunks.`);
	});

	return { jobsByChain, totalMegaChunks: chunkCounter };
}


/**
 * Mempoolã®åˆè¨ˆãƒã‚¤ãƒˆã‚µã‚¤ã‚ºãŒé–¾å€¤ã‚’ä¸‹å›ã‚‹ã¾ã§å¾…æ©Ÿã™ã‚‹ - å¤‰æ›´ãªã—
 */
async function waitForMempoolSpace(
	chainManager: ChainManager,
	chainName: string,
	bar: cliProgress.SingleBar,
	currentValue: number // ãƒãƒ¼è¡¨ç¤ºç”¨ã®ç¾åœ¨å€¤
) {
	const MEMPOOL_LIMIT_BYTES = CONFIG.MEMPOOL_BYTES_LIMIT; // ãƒã‚¤ãƒˆã‚µã‚¤ã‚ºä¸Šé™ã‚’ä½¿ç”¨
	let isReconnecting = false;
	let attempt = 0;

	while (true) {
		attempt++;
		try {
			// 1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã¨Mempoolåˆè¨ˆãƒã‚¤ãƒˆã‚µã‚¤ã‚ºå–å¾—
			logger.debug(`[${chainName}] Checking mempool space (Attempt ${attempt})...`);
			const currentBytes = await chainManager.getMempoolTotalBytes(chainName);
			logger.debug(`[${chainName}] Current mempool size: ${currentBytes} bytes`);

			if (isReconnecting) { // å†æ¥ç¶šç›´å¾Œ
				logger.info(`[${chainName}] Reconnection successful.`);
				bar.update(currentValue, { status: `Reconnected. Resuming...` });
				isReconnecting = false;
			}

			if (currentBytes < MEMPOOL_LIMIT_BYTES) {
				logger.debug(`[${chainName}] Mempool has space (${currentBytes} < ${MEMPOOL_LIMIT_BYTES}). Proceeding.`);
				return; // ç©ºãã‚ã‚Šã€æˆåŠŸ
			}

			// ç©ºããŒãªã„å ´åˆ
			const currentMB = (currentBytes / 1024 / 1024).toFixed(1);
			const limitMB = (MEMPOOL_LIMIT_BYTES / 1024 / 1024).toFixed(1);
			logger.info(`[${chainName}] Mempool full (${currentMB}/${limitMB} MB). Waiting ${CONFIG.MEMPOOL_CHECK_INTERVAL_MS}ms...`);
			bar.update(currentValue, { status: `Mempool full (${currentMB}/${limitMB} MB). Waiting...` });
			await sleep(CONFIG.MEMPOOL_CHECK_INTERVAL_MS); // sleep é–¢æ•°ã‚’ä½¿ç”¨

		} catch (e: any) {
			// å¤±æ•—ï¼šæ¥ç¶šã‚¨ãƒ©ãƒ¼ç­‰
			logger.warn(`[${chainName}] Mempool check failed (Attempt ${attempt}, Error: ${e.message}). Retrying connection...`);
			bar.update(currentValue, { status: `Connection error. Reconnecting...` });
			isReconnecting = true;
			try {
				// 2. å†æ¥ç¶šè©¦è¡Œ
				await chainManager.reconnectClient(chainName);
				// å†æ¥ç¶šæˆåŠŸã€ãƒ«ãƒ¼ãƒ—ã®æœ€åˆã«æˆ»ã£ã¦å†ãƒã‚§ãƒƒã‚¯
			} catch (reconnectError: any) {
				// 3. å†æ¥ç¶šå¤±æ•—
				logger.error(`[${chainName}] Reconnection failed. Waiting ${CONFIG.RECONNECT_DELAY_MS}ms before retry...`, reconnectError.message);
				bar.update(currentValue, { status: `Reconnect failed. Waiting...` });
				await sleep(CONFIG.RECONNECT_DELAY_MS); // sleep ã‚’ä½¿ç”¨
				// ãƒ«ãƒ¼ãƒ—ã®æœ€åˆã«æˆ»ã£ã¦å†è©¦è¡Œ
			}
		}
	}
}


/**
 * å …ç‰¢ãªãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã—ãŸãƒ¯ãƒ¼ã‚«ãƒ¼ (â˜… TxEventSubscriber ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ä¿®æ­£)
 */
async function executeDistributionWorkers(chainManager: ChainManager, megaJobsByChain: Map<string, MegaChunkJob[]>, dataChains: ChainInfo[], estimatedGas: number): Promise<void> {

	// --- ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®— (å¤‰æ›´ãªã—) ---
	const MINI_CHUNK_SIZE_BYTES = CONFIG.DEFAULT_CHUNK_SIZE;
	const ESTIMATED_ENCODED_MINI_CHUNK_SIZE = Math.ceil(MINI_CHUNK_SIZE_BYTES * 4 / 3);
	const MINI_CHUNK_SIZE_WITH_OVERHEAD = ESTIMATED_ENCODED_MINI_CHUNK_SIZE * CONFIG.TX_OVERHEAD_RATIO;
	// â˜… CONFIG.EFFECTIVE_BLOCK_SIZE_RATIO ãŒ 0.25 ã«å¤‰æ›´ã•ã‚Œã¦ã„ã‚‹
	const TARGET_BATCH_BYTES = CONFIG.BLOCK_SIZE_LIMIT_MB * 1024 * 1024 * CONFIG.EFFECTIVE_BLOCK_SIZE_RATIO;
	const DYNAMIC_BATCH_SIZE = Math.max(1, Math.floor(TARGET_BATCH_BYTES / MINI_CHUNK_SIZE_WITH_OVERHEAD));

	logger.info(`[GLOBAL_INFO] Dynamic Batch Size: ${DYNAMIC_BATCH_SIZE} TXs per batch (~${(DYNAMIC_BATCH_SIZE * MINI_CHUNK_SIZE_WITH_OVERHEAD / 1024 / 1024).toFixed(1)} MB encoded)`);
	logger.info(`[GLOBAL_INFO] Target Block Fill Ratio: ${CONFIG.EFFECTIVE_BLOCK_SIZE_RATIO * 100}% (~${(TARGET_BATCH_BYTES / 1024 / 1024).toFixed(1)} MB)`);
	logger.info(`[GLOBAL_INFO] Pipeline depth: ${CONFIG.PIPELINE_MAX_PENDING_BATCHES}`);
	logger.info(`[GLOBAL_INFO] Mempool Limit: ${(CONFIG.MEMPOOL_BYTES_LIMIT / 1024 / 1024).toFixed(1)} MB`);

	// --- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¨­å®š (å¤‰æ›´ãªã—) ---
	const multiBar = new cliProgress.MultiBar({
		clearOnComplete: false,
		hideCursor: true,
		format: '{chain} | {bar} | {percentage}% ({value}/{total}) | ETA: {eta_formatted} | TX/s: {tx_per_sec} | Status: {status} | Height: {height}',
		stream: process.stdout
	}, cliProgress.Presets.shades_grey);

	// --- ãƒ¯ãƒ¼ã‚«ãƒ¼å‡¦ç† (â˜… ä¿®æ­£) ---
	const workerPromises = dataChains.map(chain => {
		const chainName = chain.name;
		const megaJobQueue = megaJobsByChain.get(chainName)!;
		if (!megaJobQueue || megaJobQueue.length === 0) {
			logger.info(`[${chainName}] No jobs assigned, skipping worker.`);
			return Promise.resolve(); // ä»•äº‹ãŒãªã‘ã‚Œã°å³å®Œäº†
		}

		// ã“ã®ãƒã‚§ãƒ¼ãƒ³ã®ç·ãƒŸãƒ‹ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
		const totalMiniChunks = megaJobQueue.reduce((sum, job) => sum + Math.ceil(job.buffer.length / MINI_CHUNK_SIZE_BYTES), 0);
		const bar = multiBar.create(totalMiniChunks, 0, { chain: chainName.padEnd(8), tx_per_sec: '0.00', status: 'Initializing', height: 'N/A' });

		// â˜… ãƒ¯ãƒ¼ã‚«ãƒ¼ã”ã¨ã®è³¼èª­ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
		let subscriber: TxEventSubscriber | null = null;

		return (async () => { // å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã®éåŒæœŸé–¢æ•°
			let totalConfirmedTxCount = 0; // ã“ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ç¢ºèªã•ã‚ŒãŸTxç·æ•°
			try {
				const { account, tmClient } = chainManager.getClientInfo(chainName);

				// â˜… è³¼èª­ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’åˆæœŸåŒ–
				subscriber = new TxEventSubscriber(tmClient, bar, chainName);
				await subscriber.start(); // è³¼èª­ã‚’é–‹å§‹

				const messages: EncodeObject[] = [];

				// 1. å…¨ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒŸãƒ‹ãƒãƒ£ãƒ³ã‚¯ (Tx) ã«åˆ†å‰² (å¤‰æ›´ãªã—)
				logger.debug(`[${chainName}] Splitting MegaChunks into MiniChunks...`);
				for (const megaJob of megaJobQueue) {
					const megaChunkBuffer = megaJob.buffer;
					let miniOffset = 0;
					let internalChunkIndex = 0;
					while (miniOffset < megaChunkBuffer.length) {
						const miniEnd = Math.min(miniOffset + MINI_CHUNK_SIZE_BYTES, megaChunkBuffer.length);
						const miniBuffer = megaChunkBuffer.slice(miniOffset, miniEnd);
						const miniIndex = `${megaJob.indexPrefix}-mini-${internalChunkIndex}`;
						const msg = { typeUrl: '/datachain.datastore.v1.MsgCreateStoredChunk', value: { creator: account.address, index: miniIndex, data: miniBuffer }, };
						messages.push(msg);
						miniOffset = miniEnd;
						internalChunkIndex++;
					}
				}
				bar.update(0, { status: `Ready (${totalMiniChunks} TXs)` });
				logger.info(`[WORKER_START] ${chainName} ready with ${totalMiniChunks} TXs.`);

				// 2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã‚’ DYNAMIC_BATCH_SIZE ã”ã¨ã«ãƒãƒƒãƒåŒ– (å¤‰æ›´ãªã—)
				const messageBatches: EncodeObject[][] = [];
				for (let i = 0; i < messages.length; i += DYNAMIC_BATCH_SIZE) {
					messageBatches.push(messages.slice(i, i + DYNAMIC_BATCH_SIZE));
				}
				logger.info(`[WORKER_INFO] ${chainName} split into ${messageBatches.length} batches (Batch Size: ${DYNAMIC_BATCH_SIZE}).`);

				const currentSequenceRef = { sequence: 0 }; // ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ç•ªå·ç®¡ç†ç”¨
				const inclusionWaiters: Promise<Map<string, { success: boolean; height: number | undefined }>>[] = [];
				let hasFailures = false; // ãƒ¯ãƒ¼ã‚«ãƒ¼å†…ã§å¤±æ•—ãŒç™ºç”Ÿã—ãŸã‹

				// 3. å …ç‰¢ãªãƒªãƒˆãƒ©ã‚¤ä»˜ããƒãƒƒãƒå‡¦ç†ãƒ«ãƒ¼ãƒ—
				for (let batchIndex = 0; batchIndex < messageBatches.length; /* ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã¯æˆåŠŸæ™‚ã®ã¿ */) {
					const batchMessages = messageBatches[batchIndex]!;
					const BATCH_ID = `Batch ${batchIndex + 1}/${messageBatches.length}`;
					let currentBatchTxHashes: string[] = []; // ã“ã®ãƒãƒƒãƒã§é€ä¿¡ã—ãŸãƒãƒƒã‚·ãƒ¥

					try {
						// (3a) Mempoolãƒã‚§ãƒƒã‚¯ (å¤‰æ›´ãªã—)
						bar.update(totalConfirmedTxCount, { status: `${BATCH_ID} Mempool Check` });
						await waitForMempoolSpace(chainManager, chainName, bar, totalConfirmedTxCount);

						// (3b) ã‚·ãƒ¼ã‚­ãƒ³ã‚¹ã®å†å–å¾— (å¤‰æ›´ãªã—)
						if (currentSequenceRef.sequence === 0) {
							logger.info(`[${chainName}] Fetching sequence before ${BATCH_ID}...`);
							const acc = await chainManager.getClientInfo(chainName).client.getAccount(account.address);
							if (!acc) throw new Error(`Failed to get account info for ${account.address}`);
							currentSequenceRef.sequence = acc.sequence;
							logger.info(`[${chainName}] Sequence set to ${currentSequenceRef.sequence}`);
						}

						// (3c) åŒæœŸãƒãƒƒãƒé€ä¿¡ (å¤‰æ›´ãªã—)
						bar.update(totalConfirmedTxCount, { status: `${BATCH_ID} Broadcasting` });
						currentBatchTxHashes = await chainManager.broadcastSequentialTxs(
							chainName, batchMessages, estimatedGas, bar, totalConfirmedTxCount, currentSequenceRef
						);
						logger.info(`[${chainName}] ${BATCH_ID} broadcasted ${currentBatchTxHashes.length} TXs (Seq ${currentSequenceRef.sequence - currentBatchTxHashes.length} - ${currentSequenceRef.sequence - 1}).`);

						// â˜… (3d) è³¼èª­ãƒãƒãƒ¼ã‚¸ãƒ£ã«å¾…æ©Ÿã‚’ä¾é ¼
						const waiterPromise = subscriber.waitForTxs(
							currentBatchTxHashes,
							totalConfirmedTxCount, // ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©å†…ã§ãƒãƒ¼ã‚’é€²ã‚ã‚‹ãŸã‚ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ
							batchMessages.length
						);

						// (3e) çµæœå‡¦ç† (å¤‰æ›´ãªã— - totalConfirmedTxCount ã®æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ã‚‚å‰å›ä¿®æ­£æ¸ˆã¿)
						inclusionWaiters.push(waiterPromise.then(resultsMap => {
							let confirmedInThisBatch = 0;
							let failuresInThisBatch = 0;
							resultsMap.forEach((status, hash) => {
								if (hash.startsWith("ERROR_BROADCASTING")) {
								} else if (status.success && status.height) {
									confirmedInThisBatch++;
								} else {
									failuresInThisBatch++; // ç¢ºèªå¤±æ•— or ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
								}
							});

							// â˜… ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ç·ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°ã™ã‚‹
							totalConfirmedTxCount += confirmedInThisBatch;

							logger.info(`[WORKER_PIPE] ${chainName} ${BATCH_ID} finished waiting. ${confirmedInThisBatch}/${currentBatchTxHashes.length} confirmed.`);
							if (failuresInThisBatch > 0) {
								hasFailures = true; // å¤±æ•—ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
								logger.error(`[WORKER_FAIL] ${chainName} ${BATCH_ID} had ${failuresInThisBatch} failures or timed out!`);
							}
							return resultsMap;
						}));

						// (3f) èƒŒåœ§ (å¤‰æ›´ãªã—)
						if (inclusionWaiters.length >= CONFIG.PIPELINE_MAX_PENDING_BATCHES) {
							logger.debug(`[${chainName}] Pipeline full (${inclusionWaiters.length}). Waiting for a batch to complete...`);
							bar.update(totalConfirmedTxCount, { status: `Waiting (Pipeline)` });
							await inclusionWaiters.shift();
							logger.debug(`[${chainName}] Pipeline has space. Proceeding.`);
						}

						// (3g) æˆåŠŸã€‚æ¬¡ã®ãƒãƒƒãƒã¸
						batchIndex++;

					} catch (error: any) {
						// (3h) é€ä¿¡å¤±æ•—æ™‚ã®ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ (å¤‰æ›´ãªã—)
						logger.warn(`[${chainName}] Failed during ${BATCH_ID} (Broadcast/Sign?). Error: ${error.message}. Retrying...`);
						bar.update(totalConfirmedTxCount, { status: `${BATCH_ID} Failed. Retrying...` });
						currentSequenceRef.sequence = 0;
						await sleep(CONFIG.RECONNECT_DELAY_MS);
					}
				} // ãƒãƒƒãƒãƒ«ãƒ¼ãƒ— (for)

				// 4. æ®‹ã‚Šã®å¾…æ©Ÿãƒ—ãƒ­ã‚»ã‚¹ã‚’ã™ã¹ã¦å¾…ã¤ (å¤‰æ›´ãªã—)
				logger.info(`[${chainName}] All batches sent. Waiting for ${inclusionWaiters.length} final confirmations...`);
				bar.update(totalConfirmedTxCount, { status: `Final Confirmations` });
				await Promise.all(inclusionWaiters);

				// --- æœ€çµ‚çš„ãªãƒãƒ¼ã®æ›´æ–°ã¨ãƒã‚§ãƒƒã‚¯ --- (å¤‰æ›´ãªã—)
				bar.update(totalConfirmedTxCount, { status: `Finished` });

				if (hasFailures) {
					logger.error(`[WORKER_FAIL] Worker for ${chainName} finished with detected failures or timeouts.`);
					throw new Error(`Worker for ${chainName} had failures.`);
				}
				if (totalConfirmedTxCount < totalMiniChunks) {
					logger.error(`[WORKER_INCOMPLETE] Worker for ${chainName} finished but only ${totalConfirmedTxCount}/${totalMiniChunks} TXs were confirmed successfully.`);
					throw new Error(`Worker for ${chainName} did not confirm all transactions (${totalConfirmedTxCount}/${totalMiniChunks}).`);
				}

				logger.info(`[WORKER_SUCCESS] ${chainName} completed successfully.`);

			} catch (criticalError) { // ãƒ¯ãƒ¼ã‚«ãƒ¼å…¨ä½“ã® catch
				bar.stop(); // ã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒãƒ¼ã‚’åœæ­¢
				logger.error(`[CRITICAL_FAIL] Worker failed for ${chainName}. Error:`, criticalError);
				throw new Error(`Critical worker failure on chain ${chainName}: ${criticalError}`); // ã‚¨ãƒ©ãƒ¼ã‚’ä¸Šã«ä¼æ’­
			} finally {
				// â˜… (5) è³¼èª­ã‚’åœæ­¢ã™ã‚‹
				if (subscriber) {
					subscriber.stop();
				}
			}
		})(); // async IIFE
	}); // dataChains.map

	// --- å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Œäº†å¾…æ©Ÿ (å¤‰æ›´ãªã—) ---
	try {
		const results = await Promise.allSettled(workerPromises);
		const failedWorkers = results.filter(r => r.status === 'rejected');
		if (failedWorkers.length > 0) {
			logger.error(`[MAIN] ${failedWorkers.length}/${dataChains.length} workers failed.`);
			throw new Error(`${failedWorkers.length} workers failed. See logs for details.`);
		}
		logger.info("[MAIN] All workers completed successfully.");
	} finally {
		multiBar.stop(); // ã™ã¹ã¦ã®ãƒãƒ¼ã‚’åœæ­¢
	}
}


/**
 * ãƒ¡ã‚¤ãƒ³ã®åˆ†æ•£ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç† - å¤‰æ›´ãªã—
 */
async function main() {
	const chainManager = new ChainManager();
	let filePath: string | null = null;
	let totalChunksCalculated: number = 0; // è¨ˆç®—ã•ã‚ŒãŸç·ãƒãƒ£ãƒ³ã‚¯æ•°
	const startTime = Date.now();

	try {
		// 1. ç’°å¢ƒè¨­å®š (ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿/ç”Ÿæˆã€k8sãƒªã‚½ãƒ¼ã‚¹å–å¾—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–)
		const {
			filePath: fPath,
			fileBuffer,
			fileSizeInBytes, // ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚µã‚¤ã‚º
			dataChains,
			metaChain, // ä»Šå›ã¯æœªä½¿ç”¨
			megaChunkSize
		} = await setupEnvironment(chainManager);
		filePath = fPath; // ãƒ­ã‚°ç”¨

		// 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰² & ã‚¸ãƒ§ãƒ–å‰²ã‚Šå½“ã¦
		const { jobsByChain } = await createMegaChunkJobs(fileBuffer, megaChunkSize, dataChains);

		// ç·ãƒŸãƒ‹ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’äº‹å‰ã«è¨ˆç®— (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ç”¨)
		totalChunksCalculated = dataChains.reduce((total, chain) => {
			const jobs = jobsByChain.get(chain.name) || [];
			return total + jobs.reduce((sum, job) => sum + Math.ceil(job.buffer.length / CONFIG.DEFAULT_CHUNK_SIZE), 0);
		}, 0);
		logger.info(`[MAIN] Calculated total MiniChunks to upload: ${totalChunksCalculated}`);

		// 3. ã‚¬ã‚¹ä»£ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒ¼ãƒ³ã§)
		const firstDataChainName = dataChains[0]?.name;
		if (!firstDataChainName) throw new Error("No data chains available for gas simulation.");
		const firstMegaJob = jobsByChain.get(firstDataChainName)?.[0];
		if (!firstMegaJob) {
			logger.warn('[GAS_SIMULATE] No jobs assigned to the first data chain. Using fallback gas.');
			// throw new Error('No mega chunks generated for upload.');
		}

		let estimatedGas: number = 5000000; // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
		if (firstMegaJob) {
			const firstMiniChunk = firstMegaJob.buffer.subarray(0, CONFIG.DEFAULT_CHUNK_SIZE);
			const dataChainClientInfo = chainManager.getClientInfo(firstDataChainName);
			const dummyMsg: EncodeObject = {
				typeUrl: '/datachain.datastore.v1.MsgCreateStoredChunk',
				value: { creator: dataChainClientInfo.account.address, index: 'gas-simulation-dummy-0', data: firstMiniChunk },
			};

			try {
				logger.info(`[GAS_SIMULATE] Simulating gas on ${firstDataChainName}...`);
				estimatedGas = await dataChainClientInfo.client.simulate(dataChainClientInfo.account.address, [dummyMsg], 'Gas Estimation');
				logger.info(`[GAS_SIMULATE] Initial estimated gas per MiniChunk: ${estimatedGas}. Gas Wanted per TX: ${Math.round(estimatedGas * CONFIG.GAS_MULTIPLIER)}.`);
			} catch (simError) {
				logger.warn("[GAS_SIMULATE] Initial simulation failed. Using fallback gas value.", simError);
				// estimatedGas ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ã®ã¾ã¾
			}
		} else {
			logger.warn("[GAS_SIMULATE] No jobs for first chain, cannot simulate accurately. Using fallback gas.");
		}


		// 4. ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ (ã‚¤ãƒ™ãƒ³ãƒˆç‰ˆ + Mempoolãƒã‚¤ãƒˆã‚µã‚¤ã‚ºç›£è¦–)
		logger.info('[MAIN] Starting distributed chunk upload with event confirmation...');
		await executeDistributionWorkers(chainManager, jobsByChain, dataChains, estimatedGas);

		// 5. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
		logger.info(`[CLEANUP] Upload process seemingly complete for: ${filePath}.`);

	} catch (err) {
		logger.error('[MAIN] A fatal error occurred during the upload process:', err);
		throw err; // ã‚¨ãƒ©ãƒ¼ã‚’å†ã‚¹ãƒ­ãƒ¼ã—ã¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å¤±æ•—ã•ã›ã‚‹
	} finally {
		// 6. æ¥ç¶šåˆ‡æ–­
		chainManager.closeAllConnections();

		// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ã¨è¡¨ç¤º
		const endTime = Date.now();
		const totalUploadTimeMs = endTime - startTime;
		const totalUploadTimeSec = (totalUploadTimeMs / 1000).toFixed(2);
		const averageTimePerChunkMs = (totalChunksCalculated > 0 ? (totalUploadTimeMs / totalChunksCalculated) : 0).toFixed(2);
		// ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (Chunks per Second)
		const chunksPerSec = (totalChunksCalculated > 0 && totalUploadTimeMs > 0 ? (totalChunksCalculated * 1000 / totalUploadTimeMs) : 0).toFixed(2);

		// â˜… ãƒ­ã‚°ã¨åˆ†é›¢ã™ã‚‹ãŸã‚ã€æœ€çµ‚çµæœã¯æ¨™æº–å‡ºåŠ› (stdout) ã«å‡ºã™
		console.log('\n--- ğŸ“Š Distributed Upload Performance (Event Confirmation) ---');
		console.log(`Target Data Source: ${filePath}`);
		console.log(`Total Mini-Chunks Calculated: ${totalChunksCalculated}`);
		console.log(`Total Upload Time: ${totalUploadTimeSec} seconds`);
		console.log(`Average Time per Chunk: ${averageTimePerChunkMs} ms`);
		console.log(`Throughput: ${chunksPerSec} chunks/sec`);
		console.log('------------------------------------------------------------\n');

		// ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
		await loggerUtil.flushLogs();
	}
}

// å®Ÿè¡Œã¨æœ€çµ‚çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
main().then(() => {
	logger.info('[MAIN] Script finished successfully.');
	// flushLogs ã¯ finally ãƒ–ãƒ­ãƒƒã‚¯ã§å‘¼ã°ã‚Œã‚‹ã®ã§ã“ã“ã§ã¯ä¸è¦
	process.exit(0);
}).catch(err => {
	// ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ logger ã«ã‚ˆã£ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¾ã‚Œã¦ã„ã‚‹
	// æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã«ã‚‚ç°¡æ½”ã«ã‚¨ãƒ©ãƒ¼ã‚’é€šçŸ¥
	console.error(`\n[MAIN] Script execution failed. See logs for details.`);
	// flushLogs ã¯ finally ãƒ–ãƒ­ãƒƒã‚¯ã§å‘¼ã°ã‚Œã‚‹ã¯ãšã ãŒå¿µã®ãŸã‚
	loggerUtil.flushLogs().finally(() => process.exit(1));
});
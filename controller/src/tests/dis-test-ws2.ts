import { stringToPath } from '@cosmjs/crypto';
import { AccountData, DirectSecp256k1HdWallet, EncodeObject, GeneratedType, Registry, } from '@cosmjs/proto-signing';
import { calculateFee, GasPrice, IndexedTx, SigningStargateClient } from '@cosmjs/stargate';
import { Tendermint37Client, WebsocketClient } from '@cosmjs/tendermint-rpc';
import * as k8s from '@kubernetes/client-node';
import cliProgress from 'cli-progress';
import { TxRaw } from 'cosmjs-types/cosmos/tx/v1beta1/tx'; // ğŸ’¡ è¿½åŠ : TxRaw ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import * as fs from 'fs/promises';
import * as path from 'path';
import { Reader, Writer } from 'protobufjs/minimal';
import winston from 'winston';
import Transport from 'winston-transport';

// =================================================================================================
// ğŸ“š I. CONFIG & TYPE DEFINITIONS
// =================================================================================================

/**
 * ã™ã¹ã¦ã®è¨­å®šå€¤ã‚’ã“ã“ã«é›†ç´„
 */
const CONFIG = {
	K8S_NAMESPACE: 'raidchain',
	SECRET_NAME: 'raidchain-mnemonics',
	// ğŸ’¡ å¤‰æ›´ç‚¹: ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºåˆ¶é™ã‚’20MBã«è¨­å®š (Txã®æœ€å¤§ã‚µã‚¤ã‚ºæŒ‡æ¨™ã¨ã—ã¦)
	BLOCK_SIZE_LIMIT_MB: 20,
	// ğŸ’¡ å¤‰æ›´ç‚¹: 1MB (1024KB) ãƒãƒ£ãƒ³ã‚¯ã«è¨­å®š (TXãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º)
	DEFAULT_CHUNK_SIZE: 512 * 1024,
	// ğŸ’¡ æ–°è¦è¿½åŠ : ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚ã®å‹•çš„è¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
	// ğŸ’¡ ä¿®æ­£ç‚¹: ãƒ­ã‚°ã«åŸºã¥ã 0.4 (ç´„8MB) ã«å¤‰æ›´ (14ä»¶ ãŒ 7.7MB ã ã£ãŸãŸã‚)
	EFFECTIVE_BLOCK_SIZE_RATIO: 0.4, // 40%
	// Txã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (ç½²å, fee, memoç­‰) ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¦‹ç©ã‚‚ã‚Š
	TX_OVERHEAD_RATIO: 1.1, // 10%
	// ğŸ’¡ æ–°è¦è¿½åŠ : ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ¶å¾¡ (å…ˆè¡Œé€ä¿¡ã™ã‚‹ãƒãƒƒãƒæ•°)
	PIPELINE_MAX_PENDING_BATCHES: 2,
	GAS_PRICE_STRING: '0.0000001uatom',
	GAS_MULTIPLIER: 1.5,
	HD_PATH: "m/44'/118'/0'/0/2",
	MAX_RETRIES: 3,
	RETRY_BACKOFF_MS: 500,
	// ğŸ’¡ å¤‰æ›´ç‚¹: 100MB ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æƒ³å®š (5ãƒã‚§ãƒ¼ãƒ³ * 20MB)
	DEFAULT_TEST_SIZE_KB: 100 * 1024,
};

// å‹å®šç¾© (ç°¡ç•¥åŒ–ã®ãŸã‚ã«ä¸€éƒ¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’çœç•¥/anyã‚’ä½¿ç”¨)
interface TransformableInfo extends winston.Logform.TransformableInfo { level: string; message: string;[key: string]: any; }
interface StoredChunk { index: string; data: string; }
interface StoredChunkResponse { stored_chunk: StoredChunk; }
interface StoredManifestResponse { stored_manifest: { url: string; manifest: string; }; }
interface Manifest { filepath: string; chunks: { index: string; chain: string; }[]; }
interface ChainInfo { name: string; type: 'datachain' | 'metachain'; }
interface ChainEndpoints { [key: string]: string; }
interface ExtendedChainClients { client: SigningStargateClient; account: AccountData; tmClient: Tendermint37Client; wsClient: WebsocketClient; restEndpoint: string; }
// ğŸ’¡ å¤‰æ›´ç‚¹: ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã‚’è¡¨ã™ã‚¸ãƒ§ãƒ–ã¨ã€ãã®ä¸­ã«å«ã¾ã‚Œã‚‹ãƒŸãƒ‹ãƒãƒ£ãƒ³ã‚¯ã®æƒ…å ±ã‚’ä¿æŒ (ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º)
interface MegaChunkJob { buffer: Buffer; indexPrefix: string; chainName: string; retries: number; }
interface MiniChunk { index: string; data: Buffer; gasWanted: number; }

// ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒƒãƒ•ã‚¡å‹å®šç¾©ã¨ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
interface MsgCreateStoredChunk { creator: string; index: string; data: Uint8Array; }
const MsgCreateStoredChunkProto = {
	create(base?: Partial<MsgCreateStoredChunk>): MsgCreateStoredChunk { return { creator: base?.creator ?? "", index: base?.index ?? "", data: base?.data ?? new Uint8Array(), }; },
	encode(message: MsgCreateStoredChunk, writer: Writer = Writer.create()): Writer {
		if (message.creator !== '') { writer.uint32(10).string(message.creator); }
		if (message.index !== '') { writer.uint32(18).string(message.index); }
		if (message.data.length !== 0) { writer.uint32(26).bytes(message.data); }
		return writer;
	},
	decode(input: Reader | Uint8Array, length?: number): MsgCreateStoredChunk { const reader = input instanceof Reader ? input : new Reader(input); return { creator: '', index: '', data: new Uint8Array() }; },
};
interface MsgCreateStoredManifest { creator: string; url: string; manifest: string; }
const MsgCreateStoredManifestProto = {
	create(base?: Partial<MsgCreateStoredManifest>): MsgCreateStoredManifest { return { creator: base?.creator ?? "", url: base?.url ?? "", manifest: base?.manifest ?? "", }; },
	encode(message: MsgCreateStoredManifest, writer: Writer = Writer.create()): Writer {
		if (message.creator !== "") { writer.uint32(10).string(message.creator); }
		if (message.url !== "") { writer.uint32(18).string(message.url); }
		if (message.manifest !== "") { writer.uint32(26).string(message.manifest); }
		return writer;
	},
	decode(input: Reader | Uint8Array, length?: number): MsgCreateStoredManifest { const reader = input instanceof Reader ? input : new Reader(input); return { creator: "", url: "", manifest: "" }; }
};
const customRegistry = new Registry([
	['/datachain.datastore.v1.MsgCreateStoredChunk', MsgCreateStoredChunkProto as GeneratedType],
	['/metachain.metastore.v1.MsgCreateStoredManifest', MsgCreateStoredManifestProto as GeneratedType],
]);

// =================================================================================================
// ğŸ“ II. LOGGER UTILITIES (CLASS-BASED)
// =================================================================================================

class LoggerUtil {
	private readonly logBuffer: TransformableInfo[] = [];
	private readonly logger: winston.Logger;
	private readonly logFilePath: string;

	constructor() {
		const scriptFileName = path.basename(process.argv[1]!).replace(path.extname(process.argv[1]!), '');
		this.logFilePath = path.join(process.cwd(), "src/tests/", `${scriptFileName}.log`);

		class LogBufferTransport extends Transport {
			private readonly buffer: TransformableInfo[];
			constructor(buffer: TransformableInfo[], opts?: Transport.TransportStreamOptions) {
				super(opts);
				this.buffer = buffer;
			}
			log(info: any, callback: () => void) {
				setImmediate(() => { this.emit('logged', info); });
				this.buffer.push(info);
				callback();
			}
		}

		this.logger = winston.createLogger({
			level: 'info',
			format: winston.format.combine(
				winston.format.timestamp({ format: 'YYYY-MM-DDTHH:mm:ss.SSSZ' }),
				winston.format.printf(info => `[${info.timestamp}] [${info.level.toUpperCase()}] - ${info.message} ${info.stack ? '\n' + info.stack : ''}`)
			),
			transports: [
				new LogBufferTransport(this.logBuffer),
				new winston.transports.Console({
					format: winston.format.combine(
						winston.format.timestamp({ format: 'HH:mm:ss' }),
						winston.format.printf(info => `[${info.timestamp}] [${info.level.toUpperCase()}] - ${info.message}`)
					),
					level: 'info',
				})
			],
		});
	}

	public getLogger(): winston.Logger {
		return this.logger;
	}

	public async flushLogs() {
		if (this.logBuffer.length === 0) return;
		const logContent = this.logBuffer
			.map(info => {
				const transformed = this.logger.format.transform(info, {});
				return transformed && (transformed as TransformableInfo).message && info.level !== 'info' ? (transformed as TransformableInfo).message : '';
			})
			.filter(line => line.length > 0)
			.join('\n');
		try {
			await fs.writeFile(this.logFilePath, logContent + '\n', { flag: 'w' });
			console.error(`\nğŸš¨ ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ã¾ã—ãŸ: ${this.logFilePath}`);
		} catch (e) {
			console.error('ERROR: Failed to write logs to file.', e);
		}
	}
}

const loggerUtil = new LoggerUtil();
const logger = loggerUtil.getLogger();

// =================================================================================================
// ğŸ’» III. KUBERNETES UTILITIES (è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³å¯¾å¿œã«æˆ»ã™)
// =================================================================================================

const kc = new k8s.KubeConfig();
kc.loadFromDefault();
const k8sApi = kc.makeApiClient(k8s.CoreV1Api);

/**
 * Kubernetesã‹ã‚‰ãƒã‚§ãƒ¼ãƒ³æƒ…å ±ã¨REST/RPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹
 */
async function getChainResources(): Promise<{ chains: ChainInfo[], rpcEndpoints: ChainEndpoints, restEndpoints: ChainEndpoints }> {
	const resPods = await k8sApi.listNamespacedPod({
		namespace: CONFIG.K8S_NAMESPACE,
		labelSelector: 'app.kubernetes.io/component in (datachain, metachain)',
	});
	const chains: ChainInfo[] = resPods.items.map(pod => ({ name: pod.metadata!.labels!['app.kubernetes.io/instance']!, type: pod.metadata!.labels!['app.kubernetes.io/component']! as any, }));
	const rpcEndpoints: ChainEndpoints = {};
	const restEndpoints: ChainEndpoints = {};
	const isLocal = process.env.NODE_ENV !== 'production';
	const resServices = await k8sApi.listNamespacedService({
		namespace: CONFIG.K8S_NAMESPACE,
		labelSelector: "app.kubernetes.io/category=chain"
	});
	for (const chain of chains) {
		const serviceName = `raidchain-${chain.name}-headless`;
		const service = resServices.items.find(s => s.metadata?.name === serviceName);
		if (isLocal) {
			const rpcPortInfo = service?.spec?.ports?.find(p => p.name === 'rpc');
			const apiPortInfo = service?.spec?.ports?.find(p => p.name === 'api');
			if (rpcPortInfo?.nodePort) { rpcEndpoints[chain.name] = `http://localhost:${rpcPortInfo.nodePort}`; }
			if (apiPortInfo?.nodePort) { restEndpoints[chain.name] = `http://localhost:${apiPortInfo.nodePort}`; }
		} else {
			rpcEndpoints[chain.name] = `http://raidchain-${chain.name}-0.raidchain-chain-headless.${CONFIG.K8S_NAMESPACE}.svc.cluster.local:26657`;
			restEndpoints[chain.name] = `http://raidchain-${chain.name}-0.raidchain-chain-headless.${CONFIG.K8S_NAMESPACE}.svc.cluster.local:1317`;
		}
	}
	return { chains, rpcEndpoints, restEndpoints };
}

/**
 * Kubernetes Secretã‹ã‚‰ãƒ‹ãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ã‚’å–å¾—ã™ã‚‹
 */
async function getCreatorMnemonic(chainName: string): Promise<string> {
	const res = await k8sApi.readNamespacedSecret({
		name: CONFIG.SECRET_NAME,
		namespace: CONFIG.K8S_NAMESPACE,
	});
	const encodedMnemonic = res.data?.[`${chainName}.mnemonic`];
	if (!encodedMnemonic) throw new Error(`Secret does not contain mnemonic for ${chainName}.`);
	return Buffer.from(encodedMnemonic, 'base64').toString('utf-8');
}

// ğŸ’¡ ä¿®æ­£: TxãŒãƒ–ãƒ­ãƒƒã‚¯ã«å–ã‚Šè¾¼ã¾ã‚Œã‚‹ã®ã‚’å¾…ã¤é–¢æ•° (ãƒãƒ¼ãƒªãƒ³ã‚°è¨­å®šå¤‰æ›´)
// ChainManagerã®å¤–ã«å®šç¾©ã—ã€å…±é€šãƒ˜ãƒ«ãƒ‘ãƒ¼ã¨ã™ã‚‹
async function waitForTxInclusion(client: SigningStargateClient, hash: string): Promise<IndexedTx> {
	// ğŸ’¡ ä¿®æ­£: 1ç§’ãŠãã«80å› (80ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ)
	// 32ç§’ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã«å¯¾å¿œã™ã‚‹ãŸã‚
	const MAX_POLLING_ATTEMPTS = 80;
	const POLLING_INTERVAL_MS = 1000;

	for (let i = 0; i < MAX_POLLING_ATTEMPTS; i++) {
		const result = await client.getTx(hash);

		if (result) {
			if (result.code !== 0) {
				// å®Ÿè¡Œã¯å¤±æ•—ã—ãŸãŒã€ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯å«ã¾ã‚Œã¦ã„ã‚‹
				throw new Error(`Tx execution failed (Code: ${result.code}, Log: ${result.rawLog})`);
			}
			return result;
		}

		// ãƒ–ãƒ­ãƒƒã‚¯ã«å–ã‚Šè¾¼ã¾ã‚Œã‚‹ã®ã‚’å¾…ã¤ãŸã‚ã«å¾…æ©Ÿ
		await new Promise(resolve => setTimeout(resolve, POLLING_INTERVAL_MS));
	}

	throw new Error(`Transaction ${hash} was not included in a block after ${MAX_POLLING_ATTEMPTS} attempts.`);
}

// =================================================================================================
// ğŸš€ IV. CHAIN CLIENT & TRANSACTION MANAGEMENT (CLASS-BASED)
// =================================================================================================

/**
 * Cosmos SDKãƒã‚§ãƒ¼ãƒ³ã¨ã®ã‚„ã‚Šå–ã‚Šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
 */
class ChainManager {
	private readonly chainClients = new Map<string, ExtendedChainClients>();
	public readonly gasPrice: GasPrice;

	constructor() {
		this.gasPrice = GasPrice.fromString(CONFIG.GAS_PRICE_STRING);
	}

	/**
	 * ã™ã¹ã¦ã®ãƒã‚§ãƒ¼ãƒ³ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹
	 */
	public async initializeClients(allChains: ChainInfo[], rpcEndpoints: ChainEndpoints, restEndpoints: ChainEndpoints): Promise<void> {
		const initPromises = allChains.map(async (chain) => {
			try {
				const mnemonic = await getCreatorMnemonic(chain.name);
				const wallet = await DirectSecp256k1HdWallet.fromMnemonic(mnemonic, { hdPaths: [stringToPath(CONFIG.HD_PATH)] });
				const [account] = await wallet.getAccounts();
				if (!account) throw new Error(`Failed to get account from wallet for chain ${chain.name}`);

				const rpcUrl = rpcEndpoints[chain.name]!.replace('http', 'ws');
				const wsClient = new WebsocketClient(rpcUrl, (err) => { if (err) { logger.warn(`[${chain.name}] WebSocket connection error: ${err.message}`); } });
				await wsClient.execute({ jsonrpc: "2.0", method: "status", id: 1, params: [] }); // æ¥ç¶šç¢ºèª
				const tmClient = Tendermint37Client.create(wsClient);
				const client = SigningStargateClient.createWithSigner(tmClient, wallet, { registry: customRegistry, gasPrice: this.gasPrice });

				this.chainClients.set(chain.name, { client, account, tmClient, wsClient, restEndpoint: restEndpoints[chain.name]! });
				logger.info(`[CLIENT_SETUP] Successful for chain: ${chain.name} (Address: ${account.address})`);
			} catch (e) {
				logger.error(`[CLIENT_SETUP] Failed to initialize client for chain ${chain.name}:`, e);
				throw e;
			}
		});
		await Promise.all(initPromises);
	}

	public getClientInfo(chainName: string): ExtendedChainClients {
		const clientInfo = this.chainClients.get(chainName);
		if (!clientInfo) throw new Error(`Client not initialized for chain: ${chainName}`);
		return clientInfo;
	}

	// ğŸ’¡ å‰Šé™¤: broadcastSequentialTxs
	// ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã®ãŸã‚ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ (V. CORE BUSINESS LOGIC) å´ã«ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç§»å‹•


	// ğŸ’¡ æ–°è¦è¿½åŠ : Txãƒãƒƒã‚·ãƒ¥ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒ–ãƒ­ãƒƒã‚¯ã¸ã®å–ã‚Šè¾¼ã¿ã‚’å¾…æ©Ÿã™ã‚‹ (å¾…æ©Ÿå°‚ç”¨)
	/**
	 * Txãƒãƒƒã‚·ãƒ¥ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒ–ãƒ­ãƒƒã‚¯ã¸ã®å–ã‚Šè¾¼ã¿ã‚’å¾…æ©Ÿã™ã‚‹ (éåŒæœŸ)
	 * @param chainName ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒã‚§ãƒ¼ãƒ³å
	 * @param txHashes å¾…æ©Ÿã™ã‚‹Txãƒãƒƒã‚·ãƒ¥ã®é…åˆ—
	 * @param bar cliProgress.SingleBar (é€²æ—æ›´æ–°ç”¨)
	 * @param completedTxOffset ã“ã®ãƒãƒƒãƒé–‹å§‹å‰ã®å®Œäº†æ•°
	 * @param totalTxInBatch ã“ã®ãƒãƒƒãƒã®ç·æ•°
	 * @returns ãƒ–ãƒ­ãƒƒã‚¯ã«å–ã‚Šè¾¼ã¾ã‚ŒãŸå…¨ã¦ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³çµæœ (IndexedTx)
	 */
	public async waitForBatchInclusion(
		chainName: string,
		txHashes: string[],
		bar: cliProgress.SingleBar,
		completedTxOffset: number, // ã“ã®ãƒãƒƒãƒé–‹å§‹å‰ã®å®Œäº†æ•°
		totalTxInBatch: number // ã“ã®ãƒãƒƒãƒã®ç·æ•°
	): Promise<IndexedTx[]> {
		const { client } = this.getClientInfo(chainName);
		let completedTxCountInBatch = 0;
		const txStartTime = Date.now();

		const inclusionPromises = txHashes.map(hash =>
			waitForTxInclusion(client, hash) // ğŸ’¡ ãƒãƒ¼ãƒªãƒ³ã‚°è¨­å®šã‚’å¤‰æ›´ã—ãŸãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
		);

		const results = await Promise.all(inclusionPromises.map((p, index) => p.then(result => {
			completedTxCountInBatch++;
			const totalCompleted = completedTxOffset + completedTxCountInBatch;
			const txPerSec = (completedTxCountInBatch * 1000 / (Date.now() - txStartTime)).toFixed(2);

			bar.update(totalCompleted, {
				height: result.height,
				tx_per_sec: txPerSec,
				status: `Confirming (${completedTxCountInBatch}/${totalTxInBatch})`
			});
			return result;
		}).catch(e => {
			throw e;
		})));

		return results;
	}


	/**
	 * WebSocketã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã™ã¹ã¦åˆ‡æ–­ã™ã‚‹
	 */
	public closeAllConnections(): void {
		for (const { wsClient, tmClient } of this.chainClients.values()) {
			wsClient.disconnect();
			(tmClient as any).disconnect();
		}
	}
}

// =================================================================================================
// âš™ï¸ V. CORE BUSINESS LOGIC (MAIN)
// =================================================================================================

/**
 * Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œã®ã‚µã‚¤ã‚ºã‚’å…ƒã«å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
 */
function getOriginalSizeForBase64Target(targetSizeInBytes: number): number {
	// 4ãƒã‚¤ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰3ãƒã‚¤ãƒˆã®å…ƒãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã‚‹ãŸã‚ã€* 3 / 4
	return Math.floor(targetSizeInBytes * 3 / 4);
}

/**
 * ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
 */
async function setupEnvironment(chainManager: ChainManager): Promise<{
	filePath: string,
	fileSizeInBytes: number,
	dataChains: ChainInfo[],
	metaChain: ChainInfo | null,
	megaChunkSize: number
}> {
	// 1. å¼•æ•°å‡¦ç†ã¨ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
	const args = process.argv.slice(2);
	const sizeIndex = args.indexOf('--size-kb');
	const targetSizeKB = (sizeIndex !== -1 && args[sizeIndex + 1]) ? parseInt(args[sizeIndex + 1]!, 10) : CONFIG.DEFAULT_TEST_SIZE_KB;

	if (isNaN(targetSizeKB) || targetSizeKB <= 0) {
		throw new Error(`Invalid --size-kb argument: ${targetSizeKB}. Must be a positive integer.`);
	}

	const filePath = `src/tests/temp-file-${targetSizeKB}kb`;
	const originalSizeKB = Math.floor(getOriginalSizeForBase64Target(targetSizeKB * 1024) / 1024);
	const fileSizeInBytes = originalSizeKB * 1024;
	const originalContent = `This is a test file for distributed sequential upload.`;
	await fs.writeFile(filePath, Buffer.alloc(fileSizeInBytes, originalContent));
	logger.info(`[GLOBAL_INFO] Created temp file: ${filePath} (${fileSizeInBytes / 1024} KB)`);

	// 2. ç’°å¢ƒæƒ…å ±ã®å–å¾—
	const { chains: allChains, rpcEndpoints, restEndpoints: apiEndpoints } = await getChainResources();
	const dataChains = allChains.filter(c => c.type === 'datachain');
	const metaChain = allChains.find(c => c.type === 'metachain') || null;
	const numDataChains = dataChains.length;
	if (numDataChains === 0) { throw new Error('No Datachains found in Kubernetes resources.'); }

	// ğŸ’¡ ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æ±ºå®š: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º / ãƒã‚§ãƒ¼ãƒ³æ•°
	const megaChunkSize = Math.ceil(fileSizeInBytes / numDataChains);
	logger.info(`[GLOBAL_INFO] DataChains found: ${numDataChains}. Worker Chunk Size (MegaChunk) per chain: ${Math.round(megaChunkSize / 1024)} KB`);
	logger.info(`[GLOBAL_INFO] TX Chunk Size (MiniChunk): ${Math.round(CONFIG.DEFAULT_CHUNK_SIZE / 1024)} KB`);

	// 3. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
	await chainManager.initializeClients(allChains, rpcEndpoints, apiEndpoints);

	return { filePath, fileSizeInBytes, dataChains, metaChain, megaChunkSize };
}

/**
 * ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€ãƒã‚§ãƒ¼ãƒ³ã”ã¨ã®ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ã«å‰²ã‚Šå½“ã¦ã‚‹
 */
async function createMegaChunkJobs(filePath: string, megaChunkSize: number, dataChains: ChainInfo[]): Promise<{ jobsByChain: Map<string, MegaChunkJob[]>, totalMegaChunks: number }> {
	const jobsByChain = new Map<string, MegaChunkJob[]>();
	dataChains.forEach(chain => jobsByChain.set(chain.name, []));

	let chunkCounter = 0;
	const uniqueSuffix = `dist-seq-test-${Date.now()}`;
	const numDataChains = dataChains.length;

	const fileBuffer = await fs.readFile(filePath);
	let offset = 0;

	while (offset < fileBuffer.length) {
		const end = Math.min(offset + megaChunkSize, fileBuffer.length);
		const buffer = fileBuffer.slice(offset, end);

		const indexPrefix = `${uniqueSuffix}-mega-${chunkCounter}`;
		const targetChainName = dataChains[chunkCounter % numDataChains]!.name; // ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³

		const job: MegaChunkJob = { buffer: buffer, indexPrefix: indexPrefix, chainName: targetChainName, retries: 0 };
		jobsByChain.get(targetChainName)!.push(job);

		offset = end;
		chunkCounter++;
	}

	logger.info(`[ALLOCATION] File split into ${chunkCounter} MegaChunks (Worker Chunks).`);
	dataChains.forEach(chain => {
		logger.info(`[ALLOCATION] Chain ${chain.name} assigned ${jobsByChain.get(chain.name)!.length} MegaChunks.`);
	});

	return { jobsByChain, totalMegaChunks: chunkCounter };
}

/**
 * ğŸ’¡ ä¿®æ­£ç‚¹: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç† (é€ä¿¡ã¨å¾…æ©Ÿã®éåŒæœŸåŒ–) ã‚’å°å…¥
 */
async function executeDistributionWorkers(chainManager: ChainManager, megaJobsByChain: Map<string, MegaChunkJob[]>, dataChains: ChainInfo[], estimatedGas: number): Promise<void> {

	// ğŸ’¡ ä¿®æ­£ç‚¹: ãƒ­ã‚°ã«åŸºã¥ãã€å‹•çš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆä»¶æ•°ï¼‰ã‚’è¨ˆç®—
	const MINI_CHUNK_SIZE_WITH_OVERHEAD = CONFIG.DEFAULT_CHUNK_SIZE * CONFIG.TX_OVERHEAD_RATIO;
	// ğŸ’¡ ä¿®æ­£: ãƒ­ã‚°ã«åŸºã¥ã 0.4 ã«å¤‰æ›´
	const TARGET_BATCH_BYTES = CONFIG.BLOCK_SIZE_LIMIT_MB * 1024 * 1024 * CONFIG.EFFECTIVE_BLOCK_SIZE_RATIO;
	// 1ãƒ–ãƒ­ãƒƒã‚¯ã«å®‰å…¨ã«å…¥ã‚‹ã¨æ¨å®šã•ã‚Œã‚‹Txä»¶æ•°
	const DYNAMIC_BATCH_SIZE = Math.max(1, Math.floor(TARGET_BATCH_BYTES / MINI_CHUNK_SIZE_WITH_OVERHEAD));

	logger.info(`[GLOBAL_INFO] Dynamic Batch Size calculated: ${DYNAMIC_BATCH_SIZE} TXs per batch (Target: ${Math.round(TARGET_BATCH_BYTES / 1024 / 1024)}MB / Block)`);
	logger.info(`[GLOBAL_INFO] Pipeline depth (pending batches): ${CONFIG.PIPELINE_MAX_PENDING_BATCHES}`);

	const multiBar = new cliProgress.MultiBar({
		clearOnComplete: false,
		hideCursor: true,
		format: '{chain} | {bar} | {percentage}% ({value}/{total}) | {eta}s ETA | TX/s: {tx_per_sec} | Status: {status} | Height: {height}',
	}, cliProgress.Presets.shades_grey);

	const workerPromises = dataChains.map(chain => {
		const chainName = chain.name;
		const megaJobQueue = megaJobsByChain.get(chainName)!;

		// ãƒ¡ã‚¬ã‚¸ãƒ§ãƒ–ã®åˆè¨ˆãƒŸãƒ‹ãƒãƒ£ãƒ³ã‚¯æ•° (Txæ•°) ã‚’è¨ˆç®—
		const totalMiniChunks = megaJobQueue.reduce((sum, job) => sum + Math.ceil(job.buffer.length / CONFIG.DEFAULT_CHUNK_SIZE), 0);
		const bar = multiBar.create(totalMiniChunks, 0, { chain: chainName, tx_per_sec: '0.00', status: 'Pending', height: 'N/A' });

		return (async () => {
			const { client, account } = chainManager.getClientInfo(chainName);
			const messages: EncodeObject[] = [];

			// 1. å…¨ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒŸãƒ‹ãƒãƒ£ãƒ³ã‚¯ (Tx) ã«åˆ†å‰²ã—ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã‚’ä½œæˆ
			for (const megaJob of megaJobQueue) {
				const megaChunkBuffer = megaJob.buffer;
				let miniOffset = 0;
				let internalChunkIndex = 0;

				while (miniOffset < megaChunkBuffer.length) {
					const miniEnd = Math.min(miniOffset + CONFIG.DEFAULT_CHUNK_SIZE, megaChunkBuffer.length);
					const miniBuffer = megaChunkBuffer.slice(miniOffset, miniEnd);
					const miniIndex = `${megaJob.indexPrefix}-mini-${internalChunkIndex}`;

					const msg = { typeUrl: '/datachain.datastore.v1.MsgCreateStoredChunk', value: { creator: account.address, index: miniIndex, data: miniBuffer }, };
					messages.push(msg);

					miniOffset = miniEnd;
					internalChunkIndex++;
				}
			}

			bar.update(0, { status: `Total ${totalMiniChunks} TXs ready` });
			logger.info(`[WORKER_START] Worker for ${chainName} ready with ${totalMiniChunks} mini-chunks (TXs).`);

			// ğŸ’¡ å¤‰æ›´ç‚¹: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã‚’ DYNAMIC_BATCH_SIZE ã”ã¨ã«ãƒãƒƒãƒåŒ–
			const messageBatches: EncodeObject[][] = [];
			for (let i = 0; i < messages.length; i += DYNAMIC_BATCH_SIZE) {
				messageBatches.push(messages.slice(i, i + DYNAMIC_BATCH_SIZE));
			}
			logger.info(`[WORKER_INFO] ${chainName} split into ${messageBatches.length} batches (Size: ${DYNAMIC_BATCH_SIZE}).`);

			let completedTxCountInWorker = 0;

			// ğŸ’¡ å¤‰æ›´ç‚¹: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç† (é€ä¿¡ã¨å¾…æ©Ÿã‚’éåŒæœŸåŒ–)
			const inclusionWaiters: Promise<IndexedTx[]>[] = []; // å¾…æ©Ÿå°‚ç”¨ãƒªã‚¹ãƒˆ
			const chainId = await client.getChainId(); // å…ˆã«å–å¾—

			// ğŸ’¡ ä¿®æ­£: ãƒãƒ³ã‚¹æƒ…å ±ã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼å†…ã§ç®¡ç†
			const accountInfo = await client.getAccount(account.address);
			if (!accountInfo) throw new Error(`Failed to get account info for ${account.address}`);
			let currentSequence = accountInfo.sequence;
			const accountNumber = accountInfo.accountNumber;
			const gasWanted = Math.round(estimatedGas * CONFIG.GAS_MULTIPLIER);
			const fee = calculateFee(gasWanted, chainManager.gasPrice);

			try {
				for (let batchIndex = 0; batchIndex < messageBatches.length; batchIndex++) {
					const batchMessages = messageBatches[batchIndex]!;
					const batchStartTime = Date.now();
					bar.update(completedTxCountInWorker, { status: `Batch ${batchIndex + 1}/${messageBatches.length} Signing & Broadcasting` });

					// (1) ç½²åï¼†ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ (åŒæœŸå®Ÿè¡Œ)
					const txHashes: string[] = [];

					for (let i = 0; i < batchMessages.length; i++) {
						const msg = batchMessages[i]!;
						const sequence = currentSequence; // ç¾åœ¨ã®ãƒãƒ³ã‚¹

						const signedTx = await client.sign(
							account.address, [msg], fee,
							`Batch Tx (Seq: ${sequence})`,
							{ accountNumber, sequence, chainId }
						);
						const txRaw = Uint8Array.from(TxRaw.encode(signedTx).finish());

						try {
							const resultHash = await client.broadcastTxSync(txRaw);
							txHashes.push(resultHash);
							currentSequence++; // ğŸ’¡ é€ä¿¡æˆåŠŸã—ãŸã‚‰ãƒãƒ³ã‚¹ã‚’é€²ã‚ã‚‹

							// ğŸ’¡ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ã€Œé€ä¿¡ä¸­ã€ã¨ã—ã¦æ›´æ–°
							bar.update(completedTxCountInWorker + txHashes.length, { status: `Broadcasting ${txHashes.length}/${batchMessages.length}` });
						} catch (error) {
							logger.error(`[CRITICAL_FAIL] Tx (Seq ${sequence}) failed to broadcast on ${chainName}. Error:`, error);
							// å¤±æ•—ã—ãŸå ´åˆã€ãƒãƒ³ã‚¹ã¯é€²ã¾ãªã‹ã£ãŸã“ã¨ã«ãªã‚‹ã®ã§ã€æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ã‚‚åŒã˜ãƒãƒ³ã‚¹ãŒä½¿ã‚ã‚Œã‚‹ (ãŸã ã—ã€ã“ã®å®Ÿè£…ã§ã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹ã¹ã)
							throw new Error(`Broadcast failure on ${chainName}: ${error}`);
						}
					}

					// (2) å¾…æ©Ÿãƒ—ãƒ­ã‚»ã‚¹ã‚’éåŒæœŸã§é–‹å§‹
					const waiterPromise = chainManager.waitForBatchInclusion(
						chainName,
						txHashes,
						bar,
						completedTxCountInWorker, // ã‚ªãƒ•ã‚»ãƒƒãƒˆ (ã“ã®ãƒãƒƒãƒã®é–‹å§‹åœ°ç‚¹)
						batchMessages.length      // ã“ã®ãƒãƒƒãƒã®ç·æ•°
					);
					inclusionWaiters.push(waiterPromise);

					// ğŸ’¡ é€ä¿¡ãŒå®Œäº†ã—ãŸTxæ•°ã§ completedTxCountInWorker ã‚’æ›´æ–°
					completedTxCountInWorker += batchMessages.length;

					const batchTimeSec = ((Date.now() - batchStartTime) / 1000).toFixed(2);
					bar.update(completedTxCountInWorker, {
						status: `Batch ${batchIndex + 1}/${messageBatches.length} Sent (${batchTimeSec}s)`,
					});

					// (3) å¾…æ©Ÿãƒªã‚¹ãƒˆãŒæºœã¾ã‚Šã™ããŸã‚‰å¾…ã¤ (Mempool ã¸ã®èƒŒåœ§)
					// 1ãƒãƒƒãƒé€ä¿¡ (æ•°ç§’) < 1ãƒãƒƒãƒå‡¦ç† (32ç§’) ãªã®ã§ã€å¾…æ©Ÿãƒªã‚¹ãƒˆã¯æºœã¾ã£ã¦ã„ã
					if (inclusionWaiters.length >= CONFIG.PIPELINE_MAX_PENDING_BATCHES) {
						bar.update(completedTxCountInWorker, { status: `Waiting (Pipeline full)...` });
						// ğŸ’¡ ä¸€ç•ªå¤ã„ãƒãƒƒãƒã®å®Œäº†ã‚’å¾…ã¤ (shift() ã—ã¦ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤)
						await inclusionWaiters.shift();
					}
				}

				// (4) æ®‹ã‚Šã®å¾…æ©Ÿãƒ—ãƒ­ã‚»ã‚¹ã‚’ã™ã¹ã¦å¾…ã¤
				bar.update(completedTxCountInWorker, { status: 'All batches sent. Waiting for final confirmations...' });
				await Promise.all(inclusionWaiters);

				bar.update(totalMiniChunks, { status: `Finished` });

			} catch (error) {
				// ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªå¤±æ•—ã¨ã—ã¦æ‰±ã„ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’åœæ­¢
				bar.update(bar.getTotal(), { status: 'CRITICAL FAILED' });
				logger.error(`[CRITICAL_FAIL] Upload failed on ${chainName}. Error:`, error);
				throw new Error(`Critical upload failure on chain ${chainName}.`);
			}
		})();
	});

	try {
		await Promise.all(workerPromises);
	} finally {
		multiBar.stop();
	}
}

/**
 * ãƒ¡ã‚¤ãƒ³ã®åˆ†æ•£ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
 */
async function main() {
	const chainManager = new ChainManager();

	let filePath: string | null = null;
	let totalChunks: number = 0;
	let megaChunkSize: number = 0;
	let dataChains: ChainInfo[] = [];

	const startTime = Date.now();

	try {
		// 1. ç’°å¢ƒè¨­å®šã€ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
		const setup = await setupEnvironment(chainManager);
		filePath = setup.filePath;
		dataChains = setup.dataChains;
		megaChunkSize = setup.megaChunkSize;

		// 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ã‚¬ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€ãƒã‚§ãƒ¼ãƒ³ã«å‰²ã‚Šå½“ã¦
		const { jobsByChain } = await createMegaChunkJobs(filePath, megaChunkSize, dataChains);

		// 3. ã‚¬ã‚¹ä»£ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒŸãƒ‹ãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨ï¼‰
		const firstMegaJob = jobsByChain.get(dataChains[0]!.name)?.[0];
		if (!firstMegaJob) { throw new Error('No mega chunks generated for upload.'); }

		const firstMiniChunk = firstMegaJob.buffer.slice(0, CONFIG.DEFAULT_CHUNK_SIZE);

		const dataChainClient = chainManager.getClientInfo(dataChains[0]!.name);
		const dummyMsg = { typeUrl: '/datachain.datastore.v1.MsgCreateStoredChunk', value: { creator: dataChainClient.account.address, index: 'dummy-0', data: firstMiniChunk }, };
		const estimatedGas = await dataChainClient.client.simulate(dataChainClient.account.address, [dummyMsg], 'Gas Estimation');
		logger.info(`[GAS_SIMULATE] Initial estimated gas for one ${Math.round(CONFIG.DEFAULT_CHUNK_SIZE / 1024)}KB chunk: ${estimatedGas}. Gas Wanted: ${Math.round(estimatedGas * CONFIG.GAS_MULTIPLIER)}.`);

		// 4. ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ (åˆ†æ•£ä¸¦åˆ— + å†…éƒ¨ãƒãƒ³ã‚¹ã­ã˜è¾¼ã¿ + å‹•çš„ãƒãƒƒãƒå‡¦ç†)
		logger.info('[MAIN] Starting distributed sequential chunk uploads (Noncing + Pipelining via workers)...');
		await executeDistributionWorkers(chainManager, jobsByChain, dataChains, estimatedGas);

		// 5. Total Chunks ã®æœ€çµ‚è¨ˆç®—
		for (const chainName of dataChains.map(c => c.name)) {
			const megaJobQueue = jobsByChain.get(chainName)!;
			totalChunks += megaJobQueue.reduce((sum, job) => sum + Math.ceil(job.buffer.length / CONFIG.DEFAULT_CHUNK_SIZE), 0);
		}

		// 6. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
		await fs.unlink(filePath);
		logger.info(`[CLEANUP] Temporary file ${filePath} deleted.`);
		chainManager.closeAllConnections();

	} catch (err) {
		logger.error('[MAIN] A fatal error occurred:', err);
		throw err;
	} finally {
		// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬
		const endTime = Date.now();
		const totalUploadTimeMs = endTime - startTime;
		const totalUploadTimeSec = (totalUploadTimeMs / 1000).toFixed(2);

		const averageTimePerChunkMs = (totalChunks > 0 ? (totalUploadTimeMs / totalChunks) : 0).toFixed(2);

		console.log('\n--- ğŸ“Š Distributed Sequential Upload Performance ---');
		console.log(`Total Mini-Chunks Sent: ${totalChunks}`);
		console.log(`Total Upload Time: ${totalUploadTimeSec} seconds`);
		console.log(`Average Time per Chunk: ${averageTimePerChunkMs} ms`);
		console.log('--------------------------\n');
	}
}

// å®Ÿè¡Œã¨æœ€çµ‚çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
main().then(async () => {
	logger.info('[MAIN] Script finished successfully.');
	await loggerUtil.flushLogs();
	process.exit(0);
}).catch(async err => {
	logger.error('Uncaught fatal error in main execution loop:', err);
	await loggerUtil.flushLogs();
	process.exit(1);
});
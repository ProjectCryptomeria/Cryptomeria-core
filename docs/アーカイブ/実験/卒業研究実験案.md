# 実験A〜F：現コードでのデータ取得方法（コマンド例付き）＋未実装/追加が必要な点

> 前提：Kubernetes + Helm + relayer を `just deploy` / `just start-system` で起動している想定  
> Namespace はデフォルト `cryptomeria`（justfile / scripts の前提）  
> ※このリポジトリでは「/render は GWC の REST(1317) 上に生える」実装です（別HTTPサーバではない）

---

## 共通：環境準備（Pod名・port-forward・プロジェクト名）

### 1) システム起動（例）
- ビルド＆デプロイ（必要なら）
  - `just dev::build-all`
  - `just deploy 4`  # FDSC 4台など
  - `just start-system`

### 2) 変数（bash）
```bash
NS=cryptomeria

GWC_POD=$(kubectl -n $NS get pod -l app.kubernetes.io/component=gwc -o jsonpath='{.items[0].metadata.name}')
MDSC_POD=$(kubectl -n $NS get pod -l app.kubernetes.io/component=mdsc -o jsonpath='{.items[0].metadata.name}')
RELAYER_POD=$(kubectl -n $NS get pod -l app.kubernetes.io/component=relayer -o jsonpath='{.items[0].metadata.name}')

# FDSCは複数台なので一覧で取る（例）
FDSC_PODS=($(kubectl -n $NS get pod -l app.kubernetes.io/component=fdsc -o jsonpath='{.items[*].metadata.name}'))

echo "GWC=$GWC_POD"
echo "MDSC=$MDSC_POD"
echo "RELAYER=$RELAYER_POD"
echo "FDSC=${FDSC_PODS[@]}"
```

### 3) REST API へのアクセス（port-forward推奨）
```bash
# GWC API（/render がここ）
kubectl -n $NS port-forward svc/cryptomeria-gwc 1317:1317

# MDSC API（manifest取得）
kubectl -n $NS port-forward svc/cryptomeria-mdsc 1318:1317

# FDSC API（fragment取得）
# 同時に複数 port-forward するならローカルポートをずらす
# 例：fdsc-0->13170, fdsc-1->13171 ...
kubectl -n $NS port-forward svc/cryptomeria-fdsc-0 13170:1317
kubectl -n $NS port-forward svc/cryptomeria-fdsc-1 13171:1317
# ...必要台数分
```

### 4) アップロードコマンド（現コードの正しい形）
現GWC CLIは以下（`apps/gwc/x/gateway/client/cli/tx.go`）：
- `gwcd tx gateway upload [filename] [data]`
- data は `@/path/to/file` でファイル読み込み
- flags：`--project-name`, `--version`, `--fragment-size`

K8s上のGWC podで実行する例：
```bash
PROJECT="exp-$(date +%s)"
ZIP=site.zip

# zip をGWC podへコピー
kubectl -n $NS cp "$ZIP" "$GWC_POD:/tmp/$ZIP"

# upload（キー名や home は環境に依存。例として local-admin / /home/gwc/.gwc）
kubectl -n $NS exec "$GWC_POD" -- \
  gwcd tx gateway upload "$ZIP" "@/tmp/$ZIP" \
  --project-name "$PROJECT" \
  --version "v1" \
  --fragment-size 65536 \
  --from local-admin \
  --chain-id gwc \
  -y --output json \
  --keyring-backend test \
  --home /home/gwc/.gwc
```

---

# 実験A：機能成立・復元正確性（ハッシュ一致 / 成功率）

## 目的
- Zip（静的サイト等）がアップロードされ、`/render` で正しく復元できることを定量確認

## 取る指標（現実装で可能）
- 復元成功率（HTTP 200率 / ファイルごとの一致率）
- ファイルハッシュ一致（SHA-256など）
- MDSC の manifest 生成有無（プロジェクトが取得可能か）
- FDSC の fragment 取得可能性（特定 fragment_id を fetch できるか）

## 現コードでの取り方（推奨：/render を正として比較）
### (A-1) manifest が作られたか確認（MDSC REST）
```bash
# port-forward: MDSC を 1318 に割り当てている想定
curl -s "http://localhost:1318/mdsc/metastore/v1/manifest/$PROJECT" | jq .
```

### (A-2) zip 内の全ファイルを /render から取得してハッシュ比較（ローカルで実行）
```bash
# 1) ローカルでzipを展開してファイル一覧を作る
rm -rf /tmp/site && mkdir -p /tmp/site
unzip -qq "$ZIP" -d /tmp/site

# 2) すべてのファイルを列挙して、/render の戻りと sha256 を比較
#   /render は GWC API(1317) に居る想定（port-forward済み）
cd /tmp/site
find . -type f -print0 | while IFS= read -r -d '' f; do
  p="${f#./}"                       # zip内パス（実装上 cleanPath と一致する想定）
  local_hash=$(sha256sum "$f" | awk '{print $1}')

  # /render?project=...&path=...
  remote_hash=$(curl -s "http://localhost:1317/render?project=$PROJECT&path=$p" | sha256sum | awk '{print $1}')

  if [ "$local_hash" != "$remote_hash" ]; then
    echo "MISMATCH: $p local=$local_hash remote=$remote_hash"
  else
    echo "OK: $p"
  fi
done
```

## 追加実装が必要（現状だと取りにくい/無い）
- 「欠損/重複」を自動判定するための **manifest内ハッシュ**（file hash / fragment hash）が無い  
  → 卒論の実験としては、上のように外部でsha256比較は可能。ただし“システム自身の検証機構”は未実装

---

# 実験B：アップロード性能（E2E・分解・未ACK/バックプレッシャー議論）

## 目的
- アップロードの遅さ/速さを E2E と構成要素に分けて把握
- RR方式がどの条件で詰まるかを示す（混雑/不均一で顕著になるはず）

## 取る指標（現実装で可能）
- E2Eアップロード時間：`upload tx 投げる開始` → `MDSCでmanifestが取れる` まで
- goodput（payload bytes / E2E秒）
- txhash / ブロック高 / ブロック時刻（`gwcd query tx` で取得）
- relayerログ上での IBC送受信イベントのタイミング（`kubectl logs --timestamps`）

## 現コードでの取り方（E2E）
### (B-1) E2E を計測（ポーリングで manifest 出現を待つ）
```bash
PROJECT="b-$(date +%s)"
ZIP=site.zip

kubectl -n $NS cp "$ZIP" "$GWC_POD:/tmp/$ZIP"

t0=$(date +%s%3N)

# upload tx（stdoutはjson想定）
TX_JSON=$(kubectl -n $NS exec "$GWC_POD" -- \
  gwcd tx gateway upload "$ZIP" "@/tmp/$ZIP" \
  --project-name "$PROJECT" --version "v1" --fragment-size 65536 \
  --from local-admin --chain-id gwc -y --output json \
  --keyring-backend test --home /home/gwc/.gwc)

TX_HASH=$(echo "$TX_JSON" | jq -r '.txhash')
echo "tx=$TX_HASH"

# manifest が取れるまで待つ（MDSC:1318 port-forward前提）
until curl -sf "http://localhost:1318/mdsc/metastore/v1/manifest/$PROJECT" >/dev/null; do
  sleep 0.5
done

t1=$(date +%s%3N)
echo "E2E_ms=$((t1-t0))"
```

### (B-2) tx のブロック高/イベント取得（GWC pod内で query）
```bash
kubectl -n $NS exec "$GWC_POD" -- \
  gwcd query tx "$TX_HASH" -o json \
  --home /home/gwc/.gwc | jq '{height: .height, code: .code, raw_log: .raw_log}'
```

### (B-3) relayerログから「いつ動いたか」を見る（タイムスタンプ付き）
```bash
kubectl -n $NS logs "$RELAYER_POD" --timestamps --since=10m | tail -n 200
```

## 現状 “未実装/追加が必要” で取りにくいデータ（重要）
1) **アップロード処理の分解（zip展開→chunk化→sendPacket→ack待ち…）の正確な内訳**
- 現状：GWC keeper に詳細なタイムスタンプ計測ログが十分に無い  
- 対応案（実装が必要）：
  - `processZipUpload` / `ProcessZipData` / `uploadFragments` / `sendManifestPacket` の前後で
    - `t_start/t_end` を `ctx.Logger().Info` で出す（`kubectl logs --timestamps` と組み合わせて解析）
  - もしくは Prometheus メトリクスを導入（現状未導入）

2) **未ACKパケット数の推移（バックプレッシャー制御の根拠にしたい）**
- ある程度は IBC の `packet-commitments` 等の query で代替できる可能性はあるが、
  “時間推移を定量で取る”には定期ポーリング用スクリプトが必要（現状リポジトリに未用意）
- 代替の例（チャネル一覧→commitmentsを定点観測、※環境でport/channel名が変わる）：
  - `gwcd query ibc channel channels -o json`
  - `gwcd query ibc channel packet-commitments <port> <channel> -o json`
  - これを一定周期でログ化して可視化（スクリプト追加が必要）

3) **“チャンク単位のACK遅延分布（p50/p95/p99）”**
- 現状：チャンクごとの送信時刻/ACK時刻を記録していない  
- 対応案（実装が必要）：
  - 送信前に `(project, file, chunk_index, channel, fragment_id, send_time)` を記録
  - ack受信時に `(fragment_id, ack_time)` を紐づけ  
  - これで RR vs バックプレッシャー方式の比較が非常に強くなる

---

# 実験C：配信性能（/render の TTFB / Total / 断片フェッチ影響）

## 目的
- “ホスティングとしての体感”を示す（TTFB/全体時間）
- ファイルサイズや断片数・FDSC数が配信に与える影響を確認

## 取る指標（現実装で可能）
- TTFB（`time_starttransfer`）
- Total（`time_total`）
- HTTP ステータス
- （可能なら）サイズ：`size_download`

## 現コードでの取り方（curl計測）
```bash
PROJECT="c-$(date +%s)"
PATH_IN_ZIP="index.html"

# 例：TTFB/Total/Size を出す
curl -s -o /dev/null \
  -w "code=%{http_code} ttfb=%{time_starttransfer} total=%{time_total} size=%{size_download}\n" \
  "http://localhost:1317/render?project=$PROJECT&path=$PATH_IN_ZIP"
```

## 現状 “未実装/追加が必要” で取りにくいデータ
- GWC内部の「MDSC取得時間」「FDSC断片取得時間」「結合時間」の内訳ログが無い  
  → Bと同様、HTTP handler 内で計測ログ/メトリクス追加が必要
- タイムアウト/リトライ戦略が薄い（現状 `http.Get` 直呼び）ため、
  “不安定環境でのp99”を測るとハングしやすい可能性  
  → 卒論実験の安定運用のためにも `http.Client{Timeout}` 等の実装が望ましい（未実装）

---

# 実験D：スケーラビリティ（FDSC台数を増やしたときの伸び）

## 目的
- 分散保管（FDSC複数）にする価値を示す（アップロード/配信がどうスケールするか）
- “台数を増やしてどれだけ良くなるか”を定量化

## 取る指標（現実装で可能）
- B（アップロードE2E/goodput）を FDSC台数ごとに測る
- C（/render）を FDSC台数ごとに測る
- チェーンごとの格納fragment数の偏り（RRの公平性確認）

## 現コードでの取り方
### (D-1) FDSC台数の変更（Helm upgrade スクリプトあり）
```bash
# 例：FDSCを4台へ
just scale-fdsc 4

# その後、接続が必要な場合は（環境により）
just start-system
# もしくは connect-chain / connect-all 相当を実行
```

### (D-2) “各FDSCのfragment数”を取る（count_total付きで list-fragment）
```bash
# FDSC podごとに total を抜く（Autocliの list-fragment が使える）
for pod in "${FDSC_PODS[@]}"; do
  total=$(kubectl -n $NS exec "$pod" -- \
    fdscd query datastore list-fragment --pagination.limit 1 --pagination.count-total -o json \
    --home /home/fdsc/.fdsc 2>/dev/null | jq -r '.pagination.total // .pagination.total_count // empty')
  echo "$pod total=$total"
done
```
※ `pagination.total` のフィールド名は環境/バージョンで差が出ることがあるので、
うまく取れない場合は REST で `?pagination.count_total=true` を使う方法に切り替え

## 現状 “未実装/追加が必要” で取りにくいデータ
- “理想スケール（N倍でN倍速）との差”を説明するための内部ボトルネック計測（Bの分解）が不足  
  → まずは B のログ計測を入れると D の説得力が上がる

---

# 実験E：不均一・混雑環境（RRの弱点を炙り出す）

## 目的
- “遅い/混んだ FDSC が混じると RR が引っ張られる”を再現し、改善案（バックプレッシャー等）の必要性を示す

## 取る指標（現実装で可能）
- B の E2E がどれだけ劣化するか（劣化率）
- relayer ログの詰まり（遅延の兆候）
- 各FDSCの格納数・応答遅延（C）

## 現コードでの作り方（簡易：CPUを食わせて “遅いFDSC” を作る）
> ※最も手軽。ネットワーク shaping(tc/netem) は権限・ツールが必要なことが多い

```bash
# 遅くしたい fdsc pod を1つ選ぶ（例：先頭）
SLOW_FDSC="${FDSC_PODS[0]}"

# CPU hog（止めるまで回るので別ターミナル推奨）
kubectl -n $NS exec "$SLOW_FDSC" -- sh -c 'yes > /dev/null'
```

その状態で実験B（E2E）や実験C（/render）を回して比較する。

## 現状 “未実装/追加が必要” で取りにくいデータ
- ネットワークRTT/ロスを厳密に制御した比較（tc/netem）
  - これはコード未実装というより “実験環境側の準備” が必要（特権、iproute2、CNI制約）
- “straggler の影響度”を強く示すためのチャンク単位ACK遅延分布（Bで述べた計測が必要）

---

# 実験F：同時ユーザー（並列アップロード/並列配信）と公平性

## 目的
- 同時に複数ユーザーが使った時のスループット、個別完了時間、公平性を評価

## 取る指標（現実装で可能）
- 総スループット（合計 goodput）
- ユーザーごとの E2E（分布）
- /render の同時リクエスト時の p95/p99（外部ツールで測る）

## 現コードでの取り方（並列アップロード：bashで同時実行）
```bash
ZIP=site.zip
kubectl -n $NS cp "$ZIP" "$GWC_POD:/tmp/$ZIP"

N=10  # 同時アップロード数

# 各ジョブが固有の project を使う（衝突回避）
for i in $(seq 1 $N); do
  (
    PROJECT="f-$(date +%s)-$i"
    t0=$(date +%s%3N)

    kubectl -n $NS exec "$GWC_POD" -- \
      gwcd tx gateway upload "$ZIP" "@/tmp/$ZIP" \
      --project-name "$PROJECT" --version "v1" --fragment-size 65536 \
      --from local-admin --chain-id gwc -y --output json \
      --keyring-backend test --home /home/gwc/.gwc >/tmp/tx_$i.json

    # manifest 出現待ち
    until curl -sf "http://localhost:1318/mdsc/metastore/v1/manifest/$PROJECT" >/dev/null; do
      sleep 0.5
    done

    t1=$(date +%s%3N)
    echo "user=$i E2E_ms=$((t1-t0)) project=$PROJECT"
  ) &
done
wait
```

## 配信の負荷試験（外部ツール：hey/wrk など、リポジトリには同梱されていない）
```bash
# hey が入っているマシンで：
hey -n 2000 -c 50 "http://localhost:1317/render?project=$PROJECT&path=index.html"
```

## 現状 “未実装/追加が必要” で取りにくいデータ
- Jain’s fairness index などの“公平性指標”を自動計算する集計スクリプトが未同梱  
  → 上の並列アップロード結果（E2Eの配列）から後処理で計算は可能（スクリプト追加推奨）
- “ユーザーごとの帯域/未ACK量”など、バックプレッシャー議論に直結する内部統計は未計測  
  → Bで述べたチャンク単位ログ/メトリクス追加が必要

---

# 補足：バックプレッシャー・マルチバースト方式（未実装でも実験として成立させる方法）
- 現状コードでは RR のみ実装
- ただし卒論としては、以下の2段構えが現実的：
  1) **RRの弱点を実験E/Fで定量化**（不均一/混雑/同時ユーザーで劣化）
  2) **トレース駆動のオフライン評価**（未実装でも可能）
     - RR実行時のログ（送信時刻/対象FDSC/ACK時刻）を取れるように最小限のログ追加
     - そのログを入力に、バックプレッシャー/マルチバーストを“シミュレート比較”
- このアプローチなら「方式自体は未実装だが、有効性を観測データに基づき検証した」と書ける

